---
title: "Breast Cancer Project"
author: "Manuela Giansante, Liam Phan, MIcheal Bingler"
date: "2022-12-01"
output: html_document
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE,
                      cache = TRUE)

rm(list = ls())

```

```{r Libraries and functions, message=FALSE, warning= FALSE}

library(data.table) # Data table Tools
library(summarytools) # Summary Tools
library(ggplot2) # Plot Visualization Tool
library(data.table) # fread
library(rbokeh) # visualization of data
library(summarytools) # ORIGINALSummary
library(adabag) # bagging and boosting
library(caret) # pre-processing
library(dplyr) # select, mutate_if
library(fastDummies) # dummy_cols
library(splitTools) # data partition
library(rpart) # classification tree
library(rpart.plot) # plot regression trees
library(DT) # datatable
library(corrplot) # corrplot
library(gains) # gain
library(randomForest) # randomForest
library(cluster) # hierarchical clustering
library(knitr) # kable
library(kableExtra) # kbl
library(MASS) # lda, qda, etc.
library(dplyr) # Data Wrangling Tools
library(klaR) # partimat
library(forecast)
library(pROC)
library(tibble)
library(mda) # mda
library(RColorBrewer) # Color Palette
library(tidyverse) # useful Dataframe tools
library(glmnet) # Logistic Lasso Regression
library(car) # VIF 
library(ROCR) # ROC Curve
library(neuralnet) # Neural Network
library(nnet) # Neural network 
library(factoextra) # K-Means Clustering
library(ggpubr) # ggplot addons

```

```{r Confusion Matrix Function (See in References)}

# Confusion matrix
draw_confusion_matrix <- function(cm, titleaddon = '') {
  layout(matrix(c(1,1,2)))
  par(mar=c(2,2,2,2))
  plot(c(100, 345), c(300, 450), type = "n", xlab="", ylab="", xaxt='n', yaxt='n')
  title(paste0('CONFUSION MATRIX', ' ', titleaddon), cex.main=2)

  # create the matrix 
  rect(150, 430, 240, 370, col='#1c6155')
  text(195, 435, 'Benign', cex=1.2)
  rect(250, 430, 340, 370, col='#1c615570')
  text(295, 435, 'Malignant', cex=1.2)
  text(125, 370, 'Predicted', cex=1.3, srt=90, font=2)
  text(245, 450, 'Actual', cex=1.3, font=2)
  rect(150, 305, 240, 365, col='#1c615570')
  rect(250, 305, 340, 365, col='#1c6155')
  text(140, 400, 'Benign', cex=1.2, srt=90)
  text(140, 335, 'Malignant', cex=1.2, srt=90)

  # add in the cm results 
  res <- as.numeric(cm$table)
  text(195, 400, res[1], cex=1.6, font=2, col='white')
  text(195, 335, res[2], cex=1.6, font=2, col='white')
  text(295, 400, res[3], cex=1.6, font=2, col='white')
  text(295, 335, res[4], cex=1.6, font=2, col='white')

  # add in the specifics 
  plot(c(100, 0), c(100, 0), type = "n", xlab="", ylab="", main = "DETAILS", xaxt='n', yaxt='n')
  text(5, 85, names(cm$byClass[1]), cex=1.2, font=2)
  text(5, 70, round(as.numeric(cm$byClass[1]), 3), cex=1.2)
  text(23, 85, names(cm$byClass[2]), cex=1.2, font=2)
  text(23, 70, round(as.numeric(cm$byClass[2]), 3), cex=1.2)
  text(41, 85, names(cm$byClass[5]), cex=1.2, font=2)
  text(41, 70, round(as.numeric(cm$byClass[5]), 3), cex=1.2)
  text(59, 85, names(cm$byClass[6]), cex=1.2, font=2)
  text(59, 70, round(as.numeric(cm$byClass[6]), 3), cex=1.2)
  text(77, 85, names(cm$byClass[7]), cex=1.2, font=2)
  text(77, 70, round(as.numeric(cm$byClass[7]), 3), cex=1.2)
  text(95, 85, names(cm$byClass[8]), cex=1.2, font=2)
  text(95, 70, round(as.numeric(cm$byClass[8]), 3), cex=1.2)

  # add in the accuracy information 
  text(30, 35, names(cm$overall[1]), cex=1.5, font=2)
  text(30, 20, round(as.numeric(cm$overall[1]), 3), cex=1.4)
  text(70, 35, names(cm$overall[2]), cex=1.5, font=2)
  text(70, 20, round(as.numeric(cm$overall[2]), 3), cex=1.4)
}  

```

# Data Analysis


```{r Data Loading}

ORIGINAL <- fread("data/Breast_Cancer/breast-cancer.csv")

```

## Structure of Data

```{r Data Summary}

# printing sumamry
print(dfSummary(ORIGINAL, valid.col = FALSE, graph.magnif = 0.75, plain.ascii = FALSE, html = TRUE, style ='grid', silent = TRUE), max.tbl.height = 300, width = 80, method = "render")

```


## Missing values

## Distribution of Data

```{r boxplots}

# norm.value <- preProcess(ORIGINAL, method = c("center", "scale"))
# ORIGINAL.boxplot <- predict(norm.value, ORIGINAL)
# ORIGINAL.boxplot <-  melt(select(ORIGINAL.boxplot, -c(id)))
# 
# library(ggplot2)
# ggplot(ORIGINAL.boxplot, aes(x = diagnosis, y = value)) +
#   facet_wrap(~variable) +
#   stat_boxplot(geom ='errorbar') +
#   geom_boxplot()

```

## Correlation

```{r corrplot, fig.height=10, fig.width=10}

# Color Palette
library("RColorBrewer")

# select numeric variables
Corr_Data <- na.omit(ORIGINAL[,-c(1,2)])
Corr_plot <- cor(Corr_Data)

# Correlations plotting
corrplot(Corr_plot, method = "color", col=brewer.pal(n=8, name="BuGn"),tl.col="black",tl.srt=45, addCoef.col = "black",number.cex = 1)

```

# Data Preparation

## Transformation

```{r Transform}

# Transform Character Format to Binary Numerical Values on Outcome Variable "Diagnosis"

ORIGINAL[diagnosis == "M", c("diagnosis")] <- 1  # 1 for Malign Outcome
ORIGINAL[diagnosis == "B", c("diagnosis")] <- 0  # 0 for Benign Outcome 

ORIGINAL$diagnosis <- as.factor(ORIGINAL$diagnosis) # To Factor Variable 

```


## Standardization

ORIGINAL
ORIGINAL center scale
ORIGINAL range

## Partitioning 

We partition the data into Training (50%), Validation (30%) and Test (20%)

```{r Partitioning the Dataset}

set.seed(1)

# Splitting each Set from the ORIGINAL Dataset
splitting <- sample(1:3,size=nrow(ORIGINAL),replace=TRUE,prob=c(0.5,0.3,0.2))
Training <- ORIGINAL[splitting==1,]
Validation <- ORIGINAL[splitting==2,]
Test <- ORIGINAL[splitting==3,]

# Checking if proportions are right
Prop_Training <- (nrow(Training)/nrow(ORIGINAL))*100
Prop_Validation <- (nrow(Validation)/nrow(ORIGINAL))*100
Prop_Test <- (nrow(Test)/nrow(ORIGINAL))*100

# Print Proportion
paste("The Proportions are:", round(Prop_Training,2),"% In Training,",round(Prop_Validation,2),"% In Validation, and ",round(Prop_Test,2),"% In Test")

```

# Supervised Learning

## Logistic Regression

Assumptions for Logistic Regression: 

  1. The dependent variable must be categorical in nature.
  2. The independent variable should not have multi-collinearity.
  
Type of Logistic Regression: 

  1. **Binomial** (we will use this type since we only have binary outcomes, malign or benign)
  2. Multinational 
  3. Ordinal 
  
### Fit the Logistic Regression Model 

```{r Dataset Duplicata}

set.seed(1)

# Duplicate the Training and Validation Set
Training_Logistic <- Training
Validation_Logistic <- Validation

# Remove the "ID" Variable
Training_Logistic <- Training_Logistic[,-c("id")]
Validation_Logistic <- Validation_Logistic[,-c("id")]

```


```{r Fit Logistic Regression}

set.seed(1)

# Fit The Logistic Regression Model
Logistic_Model_1 <- glm(diagnosis ~ radius_mean + texture_mean + perimeter_mean + area_mean + smoothness_mean + compactness_mean + concavity_mean + `concave points_mean` + symmetry_mean + fractal_dimension_mean + radius_se + texture_se + perimeter_se + area_se + smoothness_se + compactness_se + concavity_se + `concave points_se` + symmetry_se + fractal_dimension_se, family=binomial(link='logit'), data=Training_Logistic)

# Disable Scientific Notation
options(scipen=999)

# Model Summary
summary(Logistic_Model_1)

```

> Comments: Trying to fit every variable in our logistic regression showed an convergence error when there is complete separation. To deal with this we should use a penalized model because of too many variables included in our model compared to the number of observations. (See **[Convergence Error in Logistic Regression]** and **[Penalized Logistic Regression Essentials in R: Ridge, Lasso and Elastic Net]** in References) 

### Fit a Penalized Model for Logistic Regression

There is 3 differents methods when it comes to Penalized Logistic Regression Model:

  1. **ridge regression**: variables with minor contribution have their coefficients close to zero. However, all the variables are incorporated in the model. This is useful when all variables need to be incorporated in the model according to domain knowledge.
  2. **lasso regression**: the coefficients of some less contributive variables are forced to be exactly zero. Only the most significant variables are kept in the final model.
  3. **elastic net regression**: the combination of ridge and lasso regression. It shrinks some coefficients toward zero (like ridge regression) and set some coefficients to exactly zero (like lasso regression)
  
For this case, we will use a **Lasso Regression Model** being way more strict in attributing less to no weight to variables not significant enough. 

```{r Fit Penalized Logistic Regression}

# Required Packages
library(tidyverse)
library(caret)
library(glmnet)

# Setting Seed
set.seed(1)

# Define response variable
y_lasso <- as.numeric(Training_Logistic$diagnosis)

# Define matrix of predictor variables
x_lasso <- data.matrix(Training_Logistic[,-c("diagnosis")])

# Perform k-fold cross-validation to find optimal lambda value - alpha = 1 is for using Lasso Method
cv_model <- cv.glmnet(x_lasso, y_lasso, alpha = 1)

# Find optimal lambda value that minimizes test MSE
best_lambda <- cv_model$lambda.min
print(paste("Best Lambda is equal to",best_lambda))

# Produce plot of test MSE by lambda value
plot(cv_model) 

```

> Comments: We want to use the lowest MSE and thus find the optimal Lambda. 

```{r Best Lambda for Lasso}

set.seed(1)

# Use optimal lambda value and alpha = 1 is for using Lasso Method
Logistic_Lasso_Optimal <- glmnet(x_lasso, y_lasso, alpha = 1, lambda = best_lambda)

# Disable Scientific Notation
options(scipen=999)

# Model Summary
Logistic_Lasso_Optimal$beta

```

> Comments: We can see that our Logistic Model has shrunk some variables to 0, this can be expected when using Lasso Regression, since it will get rid of unsignificant variables completely instead of setting a very low coefficient. 

### Logistic Regression using Variables from Lasso Selection

```{r Logistic Regression after Lasso}

set.seed(1)

# Fit The Logistic Regression Model with only selected variables from Lasso
Logistic_Model_After <- glm(diagnosis ~  area_mean + smoothness_mean + compactness_mean + concavity_mean + `concave points_mean` + symmetry_mean + fractal_dimension_mean + radius_se + texture_se + perimeter_se + area_se + smoothness_se + concavity_se + `concave points_se` + symmetry_se + fractal_dimension_se + radius_worst + texture_worst + area_worst + concavity_worst + `concave points_worst`+ symmetry_worst + fractal_dimension_worst , family=binomial(link='logit'), data=Training_Logistic)

# Disable Scientific Notation
options(scipen=999)

# Model Summary
summary(Logistic_Model_After)

```

> Comments: Eventhough we did a Lasso Regression, we can see that our standard Logistic Regression fails to converge with this selection of variables. This is could be due to the number of variables, 23 in our case and the low number of observations. Now it's the time to check our VIF, since Multicolinearity could be our main source of problem.

### VIF 

> We need to compute multiple stage when removing our multicolinear Variables, let's see when we don't have anymore problem of multicolinearity.

#### Test For Multicolinearity in Our Dataset using VIF - First Iteration

```{r VIF for Logistic Regression - 1}

set.seed(1)

# Load the car library
library(car)

# Create vector of VIF values
vif_values <- vif(Logistic_Model_After)

# Create horizontal bar chart to display each VIF value
barplot(vif_values, main = "VIF Values - First Iteration", horiz = FALSE, col = "steelblue", las=2)

# Add vertical line at 5
abline(h = 5, lwd = 3, lty = 2)

# Call VIF Values
vif_values

```


> Comments: We can see that most of our variables have multicolinearity (with VIF over 5). We need to remove variables with the highest VIF first. We can start by removing at least 8 variables: radius_worst, area_worst, concavity_worst, concavity_mean, concavity_worst, fractal_dimension_se, concavity_se and concave points_mean. 

#### Logistic Regression with VIF removing 8 variables - Second Iteration

```{r Logistic Regression after VIF - 2}

set.seed(1)

# Fit The Logistic Regression Model
Logistic_Model_After_VIF1 <- glm(diagnosis ~  area_mean + smoothness_mean + compactness_mean + symmetry_mean + fractal_dimension_mean + radius_se + texture_se + perimeter_se + area_se + smoothness_se + `concave points_se` + symmetry_se + texture_worst + `concave points_worst`+ symmetry_worst + fractal_dimension_worst , family=binomial(link='logit'), data=Training_Logistic)

# Disable Scientific Notation
options(scipen=999)

# Model Summary
summary(Logistic_Model_After_VIF1)

```

> Comments: Even after removing 8 variables, our model still suffer from Convergence error, let's check our VIF again.


#### Test For Multicolinearity in Our Dataset using VIF - Second Iteration

```{r VIF for Logistic Regression - 2}

set.seed(1)

# Load the car library
library(car)

# Create vector of VIF values
vif_values_2 <- vif(Logistic_Model_After_VIF1)

# Create horizontal bar chart to display each VIF value
barplot(vif_values_2, main = "VIF Values - Second Iteration", horiz = FALSE, col = "steelblue", las=2)

# Add vertical line at 5
abline(h = 5, lwd = 3, lty = 2)

# Call VIF Values
vif_values_2

```

> Comments: We can still see high VIF values in our variables, let's remove 6 variables again: radius_se, perimeter_se, area_se, compactness_mean, texture_se and texture_worst.

#### Logistic Regression with VIF removing 6 variables - Third Iteration

```{r Logistic Regression after VIF - 3}

set.seed(1)

# Fit The Logistic Regression Model
Logistic_Model_After_VIF2 <- glm(diagnosis ~  area_mean + smoothness_mean + symmetry_mean + fractal_dimension_mean + smoothness_se + `concave points_se` + symmetry_se + `concave points_worst`+ symmetry_worst + fractal_dimension_worst , family=binomial(link='logit'), data=Training_Logistic)

# Disable Scientific Notation
options(scipen=999)

# Model Summary
summary(Logistic_Model_After_VIF2)

```

> Comments: Now we can see that our Regression Model Converge, let's compute a third iteration of VIF to check the multicolinearity. 

#### Test For Multicolinearity in Our Dataset using VIF - Third Iteration

```{r VIF for Logistic Regression - 3}

set.seed(1)

# Load the car library
library(car)

# Create vector of VIF values
vif_values_3 <- vif(Logistic_Model_After_VIF2)

# Create horizontal bar chart to display each VIF value
barplot(vif_values_3, main = "VIF Values - Third Iteration", horiz = FALSE, col = "steelblue", las=2)

# Add vertical line at 5
abline(h = 5, lwd = 3, lty = 2)

# Call VIF Values
vif_values_3

```

> Comments: We are indeed improved our VIF model by excluding a lot of multicolinear variables, we can still see 3 variables suffering from a VIF higher than 5, let's remove **fractal_dimension_mean** and see if it improves everything.

#### Logistic Regression with VIF removing 1 variable - Fourth Iteration

```{r Logistic Regression after VIF - 4}

set.seed(1)

# Fit The Logistic Regression Model with only selected variables from Lasso
Logistic_Model_After_VIF3 <- glm(diagnosis ~  area_mean + smoothness_mean + symmetry_mean + smoothness_se + `concave points_se` + symmetry_se + `concave points_worst`+ symmetry_worst + fractal_dimension_worst , family=binomial(link='logit'), data=Training_Logistic)

# Disable Scientific Notation
options(scipen=999)

# Model Summary
summary(Logistic_Model_After_VIF3)

```

> Comments: Now we can see that our Regression Model Converge, let's compute a fourth iteration of VIF to check the multicolinearity. 

#### Test For Multicolinearity in Our Dataset using VIF - Fourth Iteration

```{r VIF for Logistic Regression - 4}

set.seed(1)

# Load the car library
library(car)

# Create vector of VIF values
vif_values_4 <- vif(Logistic_Model_After_VIF3)

# Create horizontal bar chart to display each VIF value
barplot(vif_values_4, main = "VIF Values - Fourth Iteration", horiz = FALSE, col = "steelblue", las=2)

# Add vertical line at 5
abline(h = 5, lwd = 3, lty = 2)

# Call VIF Values
vif_values_4

```

> Comments: We can see that our highest VIF values come from **symmetry_worst**, we can remove it and check if our model is now free of multicolinearity issues.

#### Logistic Regression with VIF removing 1 variable - Fifth Iteration (Last)

```{r Logistic Regression after VIF - 5}

set.seed(1)

# Fit The Logistic Regression Model with only selected variables from Lasso
Logistic_Model_After_VIF4 <- glm(diagnosis ~  area_mean + smoothness_mean + symmetry_mean + smoothness_se + `concave points_se` + symmetry_se + `concave points_worst` + fractal_dimension_worst , family=binomial(link='logit'), data=Training_Logistic)

# Disable Scientific Notation
options(scipen=999)

# Model Summary
summary(Logistic_Model_After_VIF4)

```

> Comments: We improved some significance level removing this high VIF variable.


#### Test For Multicolinearity in Our Dataset using VIF - Fifth Iteration (Last)


```{r VIF for Logistic Regression - 5}

set.seed(1)

# Load the car library
library(car)

# Create vector of VIF values
vif_values_5<- vif(Logistic_Model_After_VIF4)

# Create horizontal bar chart to display each VIF value
barplot(vif_values_5, main = "VIF Values - Fifth Iteration", horiz = FALSE, col = "steelblue", las=2)

# Add vertical line at 5
abline(h = 5, lwd = 3, lty = 2)

# Call VIF Values
vif_values_5

```

> Comments: Now we can see that all our selected variables are not subject to multicolinearity anymore. Let's use this model to compute some predictions.


### Predictions
#### Logistic Lasso Regression - Predictions and Confusion Matrix on Validation

```{r LR Predictions and Confusion Matrix Validation}

set.seed(1)

# Predictions with LR
Logistic_Lasso_Predictions <- predict(Logistic_Model_After_VIF4, Validation_Logistic[,c("area_mean", "smoothness_mean", "symmetry_mean", "smoothness_se", "concave points_se", "symmetry_se", "concave points_worst", "fractal_dimension_worst")], type = "response")

# Rounding Predictions - 0.5 Threshold
Logistic_Lasso_Predictions_Dummy <- round(Logistic_Lasso_Predictions)

# As Numeric
Logistic_Lasso_Predictions_Dummy <- as.numeric(Logistic_Lasso_Predictions_Dummy)

# Check rounding in a Dataframe
DF_Logistic_Lasso_Predictions <- cbind(Logistic_Lasso_Predictions, Logistic_Lasso_Predictions_Dummy)

# As Factor
Logistic_Lasso_Predictions_Dummy <- as.factor(Logistic_Lasso_Predictions_Dummy)

# Confusion Matrix
Confusion_Matrix_Logistic_Lasso <- confusionMatrix(data = Logistic_Lasso_Predictions_Dummy, reference = Validation_Logistic$diagnosis, positive = "1")

# Create the Function for Confusion Matrix
draw_confusion_matrix_Logistic_Lasso <- function(cm) {

  layout(matrix(c(1,1,2)))
  par(mar=c(2,2,2,2))
  plot(c(100, 345), c(300, 450), type = "n", xlab="", ylab="", xaxt='n', yaxt='n')
  title('CONFUSION MATRIX for Logistic Regression - Validation', cex.main=2)

  # create the matrix 
  rect(150, 430, 240, 370, col='#1c6155')
  text(195, 435, 'Benign', cex=1.2)
  rect(250, 430, 340, 370, col='#1c615570')
  text(295, 435, 'Malignant', cex=1.2)
  text(125, 370, 'Predicted', cex=1.3, srt=90, font=2)
  text(245, 450, 'Actual', cex=1.3, font=2)
  rect(150, 305, 240, 365, col='#1c615570')
  rect(250, 305, 340, 365, col='#1c6155')
  text(140, 400, 'Benign', cex=1.2, srt=90)
  text(140, 335, 'Malignant', cex=1.2, srt=90)

  # add in the cm results 
  res <- as.numeric(cm$table)
  text(195, 400, res[1], cex=1.6, font=2, col='white')
  text(195, 335, res[2], cex=1.6, font=2, col='white')
  text(295, 400, res[3], cex=1.6, font=2, col='white')
  text(295, 335, res[4], cex=1.6, font=2, col='white')

  # add in the specifics 
  plot(c(100, 0), c(100, 0), type = "n", xlab="", ylab="", main = "DETAILS", xaxt='n', yaxt='n')
  text(10, 85, names(cm$byClass[1]), cex=1.2, font=2)
  text(10, 70, round(as.numeric(cm$byClass[1]), 3), cex=1.2)
  text(30, 85, names(cm$byClass[2]), cex=1.2, font=2)
  text(30, 70, round(as.numeric(cm$byClass[2]), 3), cex=1.2)
  text(50, 85, names(cm$byClass[5]), cex=1.2, font=2)
  text(50, 70, round(as.numeric(cm$byClass[5]), 3), cex=1.2)
  text(70, 85, names(cm$byClass[6]), cex=1.2, font=2)
  text(70, 70, round(as.numeric(cm$byClass[6]), 3), cex=1.2)
  text(90, 85, names(cm$byClass[7]), cex=1.2, font=2)
  text(90, 70, round(as.numeric(cm$byClass[7]), 3), cex=1.2)

  # add in the accuracy information 
  text(30, 35, names(cm$overall[1]), cex=1.5, font=2)
  text(30, 20, round(as.numeric(cm$overall[1]), 3), cex=1.4)
  text(70, 35, names(cm$overall[2]), cex=1.5, font=2)
  text(70, 20, round(as.numeric(cm$overall[2]), 3), cex=1.4)
}

# Plot the Confusion Matrix
draw_confusion_matrix_Logistic_Lasso(Confusion_Matrix_Logistic_Lasso)

```

> Comments: 

#### ROC Curve of the Logistic Regression - Validation

```{r ROC Curve Lasso Validation}

set.seed(1)

# Load ROCR Package
library(ROCR)

# Plot our ROC Curve
pr <- ROCR::prediction(Logistic_Lasso_Predictions, Validation_Logistic$diagnosis)
prf <- ROCR::performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf, main="ROC for Validation Set")
abline(a = 0, b = 1) 

```


> Comments:

### Best Logistic Regression

```{r Logistic Regression Best Model}

# Confusion Best
Best_Logistic_Confusion <- Confusion_Matrix_Logistic_Lasso

# Predictions Best
Best_Logistic_Predictions_Dummy <- Logistic_Lasso_Predictions_Dummy
Best_Logistic_Predictions_Dummy <- factor(Best_Logistic_Predictions_Dummy)

Best_Logistic_Predictions_Probabilities <- Logistic_Lasso_Predictions

# Best Predictions as Data frame
DF_Best_Logistic_Predictions <- data.frame(Best_Logistic_Predictions_Dummy, Best_Logistic_Predictions_Probabilities) # FINAL PREDICTIONS DATAFRAME

# Best Confusion as Data frame
DF_Best_Logistic_Confusion <- data.frame(c(Best_Logistic_Confusion$byClass[c(1,2)], Best_Logistic_Confusion$overall[1]))
colnames(DF_Best_Logistic_Confusion) <- "Best Logistic Lasso Regression
DF_Best_Logistic_Confusion <- t(DF_Best_Logistic_Confusion) # FINAL CONFUSION DATAFRAME


```
 
## Classification Tree

```{r  Classification Tree 1}

set.seed(1)
# data.preparation
# so it works for me, delete later(?)
ORIGINAL_M<- data.frame(ORIGINAL) # remember to delete later
# Transform Character Format to Binary Numerical Values on Outcome Variable "Diagnosis"

# ORIGINAL_M$diagnosis <- ifelse(ORIGINAL_M$diagnosis == "M", 1, 0)
# ORIGINAL_M$diagnosis <- as.factor(ORIGINAL_M$diagnosis) # To Factor Variable
ORIGINAL_M2<- ORIGINAL_M[,-c(1)]

splitting_M <- sample(1:3,size=nrow(ORIGINAL_M2),replace=TRUE,prob=c(0.5,0.3,0.2))
Training_M <- ORIGINAL_M2[splitting_M==1,]
Validation_M <- ORIGINAL_M2[splitting_M==2,]
Test_M <- ORIGINAL_M2[splitting_M==3,]

# Checking if proportions are right
Prop_Training <- (nrow(Training_M)/nrow(ORIGINAL_M2))*100
Prop_Validation <- (nrow(Validation_M)/nrow(ORIGINAL_M2))*100
Prop_Test <- (nrow(Test_M)/nrow(ORIGINAL_M2))*100

# Print Proportion
paste("The Proportions are:", round(Prop_Training,2),"% In Training,",round(Prop_Validation,2),"% In Validation, and ",round(Prop_Test,2),"% In Test")

```

The outcome variable is a binary factor, we model a classification tree.
We first run a deep tree, with all the features included.
Then proceed to reduce the size of the deeper tree through pruning.


### run tree: 

```{r Run Tree 1, fig.width=5,fig.height=5}

set.seed(1)
options(scipen=999)

tree_full <- rpart(diagnosis ~ ., 
              data = Training_M, 
              method = "class",  # "class" because Y is a binary factor
              minbucket = 1,
              cp = 0.00001) 

# Plot tree
rpart.plot(tree_full, yesno = TRUE, digits =-6)
length(tree_full$frame$var[tree_full$frame$var == "<leaf>"]) # End nodes

relevance<-as.data.frame(tree_full$variable.importance) #we get the ranking of the variables by importance
kable(relevance, row.names = T,col.names="Variable Importance")%>% kable_paper("hover", full_width = T) #built table


printcp(tree_full, digits = 6) # print complexity value
plotcp(tree_full, upper = "splits") # we plot the progression of complexity values

#Prune the tree
min_xerr<- which.min(tree_full$cptable[,"xerror"]) # select minimum cross-validation error
cp_bp <- tree_full$cptable[min_xerr,"CP"] # find the corresponding CP value, to get the "best pruned " tree


pruned_tree<- prune(tree_full, cp = cp_bp) # re-compute the tree with the selected Cp
rpart.plot(pruned_tree, yesno = TRUE, digits =-3)
length(pruned_tree$frame$var[pruned_tree$frame$var == "<leaf>"]) # how many end nodes

```

The fully grown tree is quite reduced in size, we still pruned the tree.

### Performance of Best Pruned Tree

```{r}

# classification prediction over validation data
pruned_pred <- predict(pruned_tree, Validation_M, type = "class")
pruned_prob <- predict(pruned_tree, Validation_M, type = "prob") # probabilities of belonging to 1


# confusion matrix and accuracy of classification tree
tree_cf<- confusionMatrix(pruned_pred, Validation_M$diagnosis, positive = "1")
draw_confusion_matrix(tree_cf)

```

Sensitivity lower than Specificity ( Malign diagnosis is minority). Accuracy pretty high.

```{r}

#ROC curve
response <- data.frame(Validation_M[,1], pruned_pred) 
response$Validation_M...1.<- as.numeric(as.character(response$Validation_M...1.))
response$pruned_pred<- as.numeric(as.character(response$pruned_pred))
roc_score <- roc(data= response , response=Validation_M...1., pruned_pred) #AUC score
plot(roc_score ,main ="ROC curve")


```


## K-Nearest Neighbor

```{r data prep knn}
# partition
ORIGINAL.KNN.train <- dplyr::select(Training, -c(id))
ORIGINAL.KNN.valid <- dplyr::select(Validation, -c(id))
ORIGINAL.KNN.test <- dplyr::select(Test, -c(id))

# standardize
norm.value <- preProcess(ORIGINAL.KNN.train, method = c("center", "scale"))
ORIGINAL.KNN.train <- predict(norm.value, ORIGINAL.KNN.train)
ORIGINAL.KNN.valid <- predict(norm.value, ORIGINAL.KNN.valid)
ORIGINAL.KNN.test <- predict(norm.value, ORIGINAL.KNN.test)
```

best k 

```{r knn best k}
set.seed(1)

accuracy.df <- data.frame(k = seq(1, 30, 1), accuracy = rep(0, 30))

# iterating over different k's
for(i in 1:30){
  # nearest neighbor
  KNN1 <- knn3(y = ORIGINAL.KNN.train$diagnosis, x = dplyr::select(ORIGINAL.KNN.train, -c(diagnosis)), k = i)

  # predictions response 
  KNN1.pred.valid.resp <- predict(KNN1, dplyr::select(ORIGINAL.KNN.valid, -c(diagnosis)), type = "class")
  
  # predictions prob 
  KNN1.pred.valid.prob <- predict(KNN1, dplyr::select(ORIGINAL.KNN.valid, -c(diagnosis)), type = "prob")[,2]
  
  # Confusionmatrix
  accuracy.df[i, 2] <- confusionMatrix(KNN1.pred.valid.resp, ORIGINAL.KNN.valid$diagnosis, positive = "1")$overall[1]
}

# table in markdown
datatable(accuracy.df)

# plot the k's 
ggplot(accuracy.df) +
 aes(x = k, y = accuracy) +
 geom_line(size = 0.7, colour = "#112646") +
 labs(x = "Number of k nearest neighbours", 
 y = "Accuracy", title = "Accuracy regarding k") +
 theme_minimal()
```

calculate best knn

```{r best knn}
# nearest neighbor
KNN1 <- knn3(y = ORIGINAL.KNN.train$diagnosis, x = dplyr::select(ORIGINAL.KNN.train, -c(diagnosis)), k = 6)

# predictions response 
KNN1.pred.valid.resp <- predict(KNN1, dplyr::select(ORIGINAL.KNN.valid, -c(diagnosis)), type = "class")

# predictions prob 
KNN1.pred.valid.prob <- predict(KNN1, dplyr::select(ORIGINAL.KNN.valid, -c(diagnosis)), type = "prob")[,2]

# Confusionmatrix
KNN1.conf.mat <- confusionMatrix(KNN1.pred.valid.resp, ORIGINAL.KNN.valid$diagnosis, positive = "1")
draw_confusion_matrix(KNN1.conf.mat, titleaddon = 'KNN')
```

## Neural Networks

```{r Dataset Duplicata Neural}

set.seed(1)

# Duplicate the Training and Validation Set
Training_NN <- Training
Validation_NN <- Validation
Test_NN <- Test

# Make Sure to be as Dataframe
Training_NN <- data.frame(Training_NN)
Validation_NN <- data.frame(Validation_NN)
Test_NN <- data.frame(Test_NN)

# Remove the "ID" Variable
Training_NN <- Training_NN[,-1]
Validation_NN <- Validation_NN[,-1]
Test_NN <- Test_NN[,-1]

# Preprocess Data
Norm_NN <- preProcess(Training_NN, method = c("center", "scale"))
Training_NN_Preprocess <- predict(Norm_NN, Training_NN)
Validation_NN_Preprocess <- predict(Norm_NN, Validation_NN)
Test_NN_Preprocess <- predict(Norm_NN, Test_NN)

```

### Basic Neural Network with 3 hidden nodes - Neural Model 1

<center>

```{r Neural Network - Model 1, fig.height=6, fig.width=10}

set.seed(1)

# Load library
library(neuralnet)
library(nnet)

# Fit neural network with 3 hidden layers
Neural_1 <- neuralnet(diagnosis ~ ., data = Training_NN_Preprocess, hidden=3, linear.output = FALSE)

# Plot Neural Network Model 1
(plot(Neural_1))

```

</center>


#### Predictions and Confusion Matrix - Neural Model 1

#### Confusion Matrix with Validation

```{r Predictions and Confusion Matrix for NN 1}

set.seed(1)

# Predictions
Predictions_NN1 <- predict(Neural_1, Validation_NN_Preprocess, type="response")
Predictions_NN1_Probabilities <- Predictions_NN1[,2]

# Rounding Predictions - 0.5 Threshold
Predictions_NN1_Dummy <- round(Predictions_NN1_Probabilities)

# As Numeric
Predictions_NN1_Dummy <- as.numeric(Predictions_NN1_Dummy)

# Check rounding in a Dataframe
DF_Neural_Predictions <- cbind(Predictions_NN1_Probabilities, Predictions_NN1_Dummy)

# As Factor
Predictions_NN1_Dummy <- factor(Predictions_NN1_Dummy)
Validation_NN_Preprocess$diagnosis <- factor(Validation_NN_Preprocess$diagnosis)

# Confusion Matrix
Confusion_Matrix_Neural_1 <- confusionMatrix(data = Predictions_NN1_Dummy, reference = Validation_NN_Preprocess$diagnosis, positive = "1")

# Create the Function for Confusion Matrix
draw_confusion_matrix_Neural_1 <- function(cm) {

  layout(matrix(c(1,1,2)))
  par(mar=c(2,2,2,2))
  plot(c(100, 345), c(300, 450), type = "n", xlab="", ylab="", xaxt='n', yaxt='n')
  title('CONFUSION MATRIX for Neural Network - Model 1', cex.main=2)

  # create the matrix 
  rect(150, 430, 240, 370, col='#1c6155')
  text(195, 435, 'Benign', cex=1.2)
  rect(250, 430, 340, 370, col='#1c615570')
  text(295, 435, 'Malignant', cex=1.2)
  text(125, 370, 'Predicted', cex=1.3, srt=90, font=2)
  text(245, 450, 'Actual', cex=1.3, font=2)
  rect(150, 305, 240, 365, col='#1c615570')
  rect(250, 305, 340, 365, col='#1c6155')
  text(140, 400, 'Benign', cex=1.2, srt=90)
  text(140, 335, 'Malignant', cex=1.2, srt=90)

  # add in the cm results 
  res <- as.numeric(cm$table)
  text(195, 400, res[1], cex=1.6, font=2, col='white')
  text(195, 335, res[2], cex=1.6, font=2, col='white')
  text(295, 400, res[3], cex=1.6, font=2, col='white')
  text(295, 335, res[4], cex=1.6, font=2, col='white')

  # add in the specifics 
  plot(c(100, 0), c(100, 0), type = "n", xlab="", ylab="", main = "DETAILS", xaxt='n', yaxt='n')
  text(10, 85, names(cm$byClass[1]), cex=1.2, font=2)
  text(10, 70, round(as.numeric(cm$byClass[1]), 3), cex=1.2)
  text(30, 85, names(cm$byClass[2]), cex=1.2, font=2)
  text(30, 70, round(as.numeric(cm$byClass[2]), 3), cex=1.2)
  text(50, 85, names(cm$byClass[5]), cex=1.2, font=2)
  text(50, 70, round(as.numeric(cm$byClass[5]), 3), cex=1.2)
  text(70, 85, names(cm$byClass[6]), cex=1.2, font=2)
  text(70, 70, round(as.numeric(cm$byClass[6]), 3), cex=1.2)
  text(90, 85, names(cm$byClass[7]), cex=1.2, font=2)
  text(90, 70, round(as.numeric(cm$byClass[7]), 3), cex=1.2)

  # add in the accuracy information 
  text(30, 35, names(cm$overall[1]), cex=1.5, font=2)
  text(30, 20, round(as.numeric(cm$overall[1]), 3), cex=1.4)
  text(70, 35, names(cm$overall[2]), cex=1.5, font=2)
  text(70, 20, round(as.numeric(cm$overall[2]), 3), cex=1.4)
}

# Plot the Confusion Matrix
draw_confusion_matrix_Neural_1(Confusion_Matrix_Neural_1)

```

> Comments: Lowest Accuracy on Validation and Specificity.


### Advanced Neural Network with 1 Hidden Layers of 15 Hidden Nodes - Model 2

Some rules of thumb can help in deciding the structure of our Neural Network Model, here are some from Stackoverflow and the book **Introduction to Neural Networks for Java (second edition)** by Jeff Heaton:

For **Hidden Neurons**:
    "1. The number of hidden neurons should be between the size of the input layer and the size of the output layer.
    2. The number of hidden neurons should be 2/3 the size of the input layer, plus the size of the output layer.
    3. The number of hidden neurons should be less than twice the size of the input layer."

Since we have 30 input nodes (30 features/variables) and 2 output nodes (2 Classes with variable **diagnosis**), 15 hidden neurons sound like the number to go. 
  
For **Hidden Layers**: "Problems that require two hidden layers are rarely encountered. However, neural networks with two hidden layers can represent functions with any kind of shape. There is currently no theoretical reason to use neural networks with any more than two hidden layers. In fact, for many practical problems, there is no reason to use any more than one hidden layer." Thus we can use 1 layers and later see if 2 layers still be relevant compared to our validation and test set.

<center>

```{r Neural Network - Model 2, fig.width=10, fig.height=6}

set.seed(1)

# Load library
library(neuralnet)
library(nnet)
library(devtools)

# Fit neural network with 15 hidden nodes
Neural_2 <- neuralnet(diagnosis ~ ., data = Training_NN_Preprocess, hidden=15, linear.output = FALSE)

# Plot Neural Network Model 2
(plot(Neural_2))

```

</center>

#### Predictions and Confusion Matrix - Neural Model 2
##### Confusion Matrix with Validation

<center>

```{r Predictions and Confusion Matrix for NN 2}

set.seed(1)

# Predictions 
Predictions_NN2 <- predict(Neural_2, Validation_NN_Preprocess, type="response")
Predictions_NN2_Probabilities <- Predictions_NN2[,2]

# Rounding Predictions - 0.5 Threshold 
Predictions_NN2_Dummy <- round(Predictions_NN2_Probabilities)

# As Numeric
Predictions_NN2_Dummy <- as.numeric(Predictions_NN2_Dummy)

# Check rounding in a Dataframe
DF_Neural_Predictions_2 <- cbind(Predictions_NN2_Probabilities, Predictions_NN2_Dummy)

# As Factor
Predictions_NN2_Dummy <- factor(Predictions_NN2_Dummy)
Validation_NN_Preprocess$diagnosis <- factor(Validation_NN_Preprocess$diagnosis)

# Confusion Matrix
Confusion_Matrix_Neural_2 <- confusionMatrix(data = Predictions_NN2_Dummy, reference = Validation_NN_Preprocess$diagnosis, positive = "1")

# Create the Function for Confusion Matrix
draw_confusion_matrix_Neural_2 <- function(cm) {

  layout(matrix(c(1,1,2)))
  par(mar=c(2,2,2,2))
  plot(c(100, 345), c(300, 450), type = "n", xlab="", ylab="", xaxt='n', yaxt='n')
  title('CONFUSION MATRIX for Neural Network - Model 2', cex.main=2)

  # create the matrix 
  rect(150, 430, 240, 370, col='#1c6155')
  text(195, 435, 'Benign', cex=1.2)
  rect(250, 430, 340, 370, col='#1c615570')
  text(295, 435, 'Malignant', cex=1.2)
  text(125, 370, 'Predicted', cex=1.3, srt=90, font=2)
  text(245, 450, 'Actual', cex=1.3, font=2)
  rect(150, 305, 240, 365, col='#1c615570')
  rect(250, 305, 340, 365, col='#1c6155')
  text(140, 400, 'Benign', cex=1.2, srt=90)
  text(140, 335, 'Malignant', cex=1.2, srt=90)

  # add in the cm results 
  res <- as.numeric(cm$table)
  text(195, 400, res[1], cex=1.6, font=2, col='white')
  text(195, 335, res[2], cex=1.6, font=2, col='white')
  text(295, 400, res[3], cex=1.6, font=2, col='white')
  text(295, 335, res[4], cex=1.6, font=2, col='white')

  # add in the specifics 
  plot(c(100, 0), c(100, 0), type = "n", xlab="", ylab="", main = "DETAILS", xaxt='n', yaxt='n')
  text(10, 85, names(cm$byClass[1]), cex=1.2, font=2)
  text(10, 70, round(as.numeric(cm$byClass[1]), 3), cex=1.2)
  text(30, 85, names(cm$byClass[2]), cex=1.2, font=2)
  text(30, 70, round(as.numeric(cm$byClass[2]), 3), cex=1.2)
  text(50, 85, names(cm$byClass[5]), cex=1.2, font=2)
  text(50, 70, round(as.numeric(cm$byClass[5]), 3), cex=1.2)
  text(70, 85, names(cm$byClass[6]), cex=1.2, font=2)
  text(70, 70, round(as.numeric(cm$byClass[6]), 3), cex=1.2)
  text(90, 85, names(cm$byClass[7]), cex=1.2, font=2)
  text(90, 70, round(as.numeric(cm$byClass[7]), 3), cex=1.2)

  # add in the accuracy information 
  text(30, 35, names(cm$overall[1]), cex=1.5, font=2)
  text(30, 20, round(as.numeric(cm$overall[1]), 3), cex=1.4)
  text(70, 35, names(cm$overall[2]), cex=1.5, font=2)
  text(70, 20, round(as.numeric(cm$overall[2]), 3), cex=1.4)
}

# Plot the Confusion Matrix
draw_confusion_matrix_Neural_2(Confusion_Matrix_Neural_2)

```

</center>

> Comments: BEST MODEL both Sensitivity and Accuracy

#### ROC Curve for Neural Model 2 

##### On Validation

```{r ROC Curve NN Validation}

set.seed(1)

# Load ROCR Package
library(ROCR)

# Plot our ROC Curve
pr2_NN <- ROCR::prediction(Predictions_NN2_Probabilities, Validation_NN_Preprocess$diagnosis)
prf2_NN <- ROCR::performance(pr2_NN, measure = "tpr", x.measure = "fpr")
plot(prf2_NN, main="ROC for Validation Set")
abline(a = 0, b = 1) 

```




### Advanced Neural Network with 2 Hidden Layers of 15 Hidden Nodes - Model 3

<center>

```{r Neural Network - Model 3, fig.height=6, fig.width=10}

set.seed(1)

# Load library
library(neuralnet)
library(nnet)
library(devtools)

# Fit neural network with 15 hidden nodes
Neural_3 <- neuralnet(diagnosis ~ ., data = Training_NN_Preprocess, hidden=c(15,15), linear.output = FALSE)

# Plot Neural Network Model 2
(plot(Neural_3))

```

</center>

#### Predictions and Confusion Matrix - Neural Model 3

##### Confusion Matrix For Validation

<center>

```{r Predictions and Confusion Matrix for NN 3}

set.seed(1)

# Predictions 
Predictions_NN3 <- predict(Neural_3, Validation_NN_Preprocess, type="response")
Predictions_NN3_Probabilities <- Predictions_NN3[,2]

# Rounding Predictions - 0.5 Threshold 
Predictions_NN3_Dummy <- round(Predictions_NN3_Probabilities)

# As Numeric
Predictions_NN3_Dummy <- as.numeric(Predictions_NN3_Dummy)

# Check rounding in a Dataframe
DF_Neural_Predictions_3 <- cbind(Predictions_NN3_Probabilities, Predictions_NN3_Dummy)

# As Factor
Predictions_NN3_Dummy <- factor(Predictions_NN3_Dummy)
Validation_NN_Preprocess$diagnosis <- factor(Validation_NN_Preprocess$diagnosis)

# Confusion Matrix
Confusion_Matrix_Neural_3 <- confusionMatrix(data = Predictions_NN3_Dummy, reference = Validation_NN_Preprocess$diagnosis, positive = "1")

# Create the Function for Confusion Matrix
draw_confusion_matrix_Neural_3 <- function(cm) {

  layout(matrix(c(1,1,2)))
  par(mar=c(2,2,2,2))
  plot(c(100, 345), c(300, 450), type = "n", xlab="", ylab="", xaxt='n', yaxt='n')
  title('CONFUSION MATRIX for Neural Network - Model 3', cex.main=2)

  # create the matrix 
  rect(150, 430, 240, 370, col='#1c6155')
  text(195, 435, 'Benign', cex=1.2)
  rect(250, 430, 340, 370, col='#1c615570')
  text(295, 435, 'Malignant', cex=1.2)
  text(125, 370, 'Predicted', cex=1.3, srt=90, font=2)
  text(245, 450, 'Actual', cex=1.3, font=2)
  rect(150, 305, 240, 365, col='#1c615570')
  rect(250, 305, 340, 365, col='#1c6155')
  text(140, 400, 'Benign', cex=1.2, srt=90)
  text(140, 335, 'Malignant', cex=1.2, srt=90)

  # add in the cm results 
  res <- as.numeric(cm$table)
  text(195, 400, res[1], cex=1.6, font=2, col='white')
  text(195, 335, res[2], cex=1.6, font=2, col='white')
  text(295, 400, res[3], cex=1.6, font=2, col='white')
  text(295, 335, res[4], cex=1.6, font=2, col='white')

  # add in the specifics 
  plot(c(100, 0), c(100, 0), type = "n", xlab="", ylab="", main = "DETAILS", xaxt='n', yaxt='n')
  text(10, 85, names(cm$byClass[1]), cex=1.2, font=2)
  text(10, 70, round(as.numeric(cm$byClass[1]), 3), cex=1.2)
  text(30, 85, names(cm$byClass[2]), cex=1.2, font=2)
  text(30, 70, round(as.numeric(cm$byClass[2]), 3), cex=1.2)
  text(50, 85, names(cm$byClass[5]), cex=1.2, font=2)
  text(50, 70, round(as.numeric(cm$byClass[5]), 3), cex=1.2)
  text(70, 85, names(cm$byClass[6]), cex=1.2, font=2)
  text(70, 70, round(as.numeric(cm$byClass[6]), 3), cex=1.2)
  text(90, 85, names(cm$byClass[7]), cex=1.2, font=2)
  text(90, 70, round(as.numeric(cm$byClass[7]), 3), cex=1.2)

  # add in the accuracy information 
  text(30, 35, names(cm$overall[1]), cex=1.5, font=2)
  text(30, 20, round(as.numeric(cm$overall[1]), 3), cex=1.4)
  text(70, 35, names(cm$overall[2]), cex=1.5, font=2)
  text(70, 20, round(as.numeric(cm$overall[2]), 3), cex=1.4)
}

# Plot the Confusion Matrix
draw_confusion_matrix_Neural_3(Confusion_Matrix_Neural_3)

```

</center>

> Comments: NOT BETTER


### Neural Network Best Model

```{r Neural Network Best Model}

# Confusion Best
Best_Neural_Network_Confusion <- Confusion_Matrix_Neural_2

# Predictions Best
Best_Neural_Network_Predictions_Dummy <- Predictions_NN2_Dummy
Best_Neural_Network_Predictions_Dummy <- factor(Best_Neural_Network_Predictions_Dummy)

Best_Neural_Network_Predictions_Probabilities <- Predictions_NN2_Probabilities

# Best Predictions as Data frame
DF_Best_Neural_Network_Predictions <- data.frame(Best_Neural_Network_Predictions_Dummy, Best_Neural_Network_Predictions_Probabilities) # FINAL PREDICTIONS DATAFRAME

# Best Confusion as Data frame
DF_Best_Neural_Confusion <- data.frame(c(Best_Neural_Network_Confusion$byClass[c(1,2)], Best_Neural_Network_Confusion$overall[1]))
colnames(DF_Best_Neural_Confusion) <- "Best Neural Network"
DF_Best_Neural_Confusion <- t(DF_Best_Neural_Confusion) # FINAL CONFUSION DATAFRAME

```

## Discriminant Analysis

First I just run a discriminant analysis.

First data prep before model -> center / scale, 

```{r DA data prep}
set.seed(1)

# without outliers
# ORIGINAL.DA.WOO <- tibble::tibble(ORIGINAL.DA)
# 
# library(tidyverse)
# 
# ORIGINAL.DA.WOO <- ORIGINAL.DA.WOO %>%
#   dplyr::select(-c(diagnosis)) %>%
#   gather(key, value) %>%
#   group_by(key) %>%
#   mutate(q95 = quantile(value, 0.95), row = row_number()) %>%
#   filter(value <= q95) %>%
#   dplyr::select(-q95) %>%
#   spread(key, value) %>%
#   dplyr::select(-row)
                                   
# partition
ORIGINAL.DA.train <- dplyr::select(Training, -c(id))
ORIGINAL.DA.valid <- dplyr::select(Validation, -c(id))
ORIGINAL.DA.test <- dplyr::select(Test, -c(id))

# standardize
norm.value <- preProcess(ORIGINAL.DA.train, method = c("center", "scale"))
ORIGINAL.DA.train <- predict(norm.value, ORIGINAL.DA.train)
ORIGINAL.DA.valid <- predict(norm.value, ORIGINAL.DA.valid)
ORIGINAL.DA.test <- predict(norm.value, ORIGINAL.DA.test)
```

try with lda

```{r first DA}
set.seed(1)

# Fit the model
DA1 <- lda(diagnosis~., data = ORIGINAL.DA.train)

# Make predictions
predictions <- predict(DA1, ORIGINAL.DA.valid)

# predictions prob
DA1.pred.valid.prob <- predictions$posterior[,2]

# predictions response 
DA1.pred.valid.resp <- factor(predictions$class)

# confusion matrix
DA1.conf.mat <- confusionMatrix(DA1.pred.valid.resp, ORIGINAL.DA.valid$diagnosis, positive = "1")
draw_confusion_matrix(DA1.conf.mat, titleaddon = 'Discriminant Analysis')

# Evaluating LDA
# doesn't work
# klaR::partimat(diagnosis~., data = ORIGINAL.DA.train, method="lda", plot.matrix = FALSE)
```

If bad -> Correlation in data -> qda continuing with it. If not continue with lda

```{r qda}
set.seed(1)

# Fit the model
DA2 <- qda(diagnosis~., data = ORIGINAL.DA.train)

# Make predictions
predictions <- predict(DA2, ORIGINAL.DA.valid)

# predictions prob
DA2.pred.valid.prob <- predictions$posterior[,2]

# predictions response 
DA2.pred.valid.resp <- factor(predictions$class)

# confusion matrix
DA2.conf.mat <- confusionMatrix(DA2.pred.valid.resp, ORIGINAL.DA.valid$diagnosis, positive = "1")
draw_confusion_matrix(DA2.conf.mat, titleaddon = 'Quadratic Discriminant Analysis')
```

Doesn't get better... lets continue with lda

We see different proportions -> need to refit model with cost to proportion 

```{r lda different proportions}
prior = DA1$prior

# Fit the model
DA3 <- lda(diagnosis~., data = ORIGINAL.DA.train, prior = prior)

# Make predictions
predictions <- predict(DA3, ORIGINAL.DA.valid)

# predictions prob
DA3.pred.valid.prob <- predictions$posterior[,2]

# predictions response 
DA3.pred.valid.resp <- factor(predictions$class)

# confusion matrix
DA3.conf.mat <- confusionMatrix(DA3.pred.valid.resp, ORIGINAL.DA.valid$diagnosis, positive = "1")
draw_confusion_matrix(DA3.conf.mat, titleaddon = 'Discriminant Analsis with priors')
```
Gives back same model as without prior -> function already does it.

Try mda

```{r mda}
set.seed(1)

# Fit the model
DA4 <- mda(diagnosis~., data = ORIGINAL.DA.train)

# predictions prob
DA4.pred.valid.prob <- predict(DA4, ORIGINAL.DA.valid, type = "posterior")[,2]

# predictions response 
DA4.pred.valid.resp <- factor(ifelse(DA4.pred.valid.prob > 0.5, 1, 0))

# confusion matrix
DA4.conf.mat <- confusionMatrix(DA4.pred.valid.resp, ORIGINAL.DA.valid$diagnosis, positive = "1")
draw_confusion_matrix(DA4.conf.mat, 'Mixture discriminant analysis')
```

try fda

```{r fda}
set.seed(1)

# Fit the model
DA5 <- fda(diagnosis~., data = ORIGINAL.DA.train)

# predictions prob
DA5.pred.valid.prob <- predict(DA5, ORIGINAL.DA.valid, type = "posterior")[,2]

# predictions response 
DA5.pred.valid.resp <- factor(ifelse(DA5.pred.valid.prob > 0.5, 1, 0))

# confusion matrix
DA5.conf.mat <- confusionMatrix(DA5.pred.valid.resp, ORIGINAL.DA.valid$diagnosis, positive = "1")
draw_confusion_matrix(DA5.conf.mat, titleaddon = 'Flexible discriminant analysis')
```

highest sensitivity is for qda --> sensitivity is important in this case. 


We can skip this stuff. Talked with others thex didn't do this stuff... especially because our lda / qda works fine. Could potentially run qda with minimal predictors from logistic regression as they work kinda similar. 



we know missclassification bad -> cost for misclassification model

3rd model

We see that outliers make it bad -> remove some rows

4th model

Some predictors unimportant -> remove some

5th model


## Ensemble Methods

### bagging

```{r}
set.seed(1)
bagging<- bagging(diagnosis ~ ., data =Training_M)
bag_pred<- predict(bagging, Validation_M, type="class")
bag_cf <- confusionMatrix(as.factor(bag_pred$class), Validation_M$diagnosis, positive = "1")
draw_confusion_matrix(bag_cf)

```


### boosting

```{r}
set.seed(1)
boosting <- boosting(diagnosis ~ ., data = Training_M)
boost_pred<- predict(boosting, Validation_M, type="class")
boost_cf <- confusionMatrix(as.factor(boost_pred$class), Validation_M$diagnosis, positive = "1")
draw_confusion_matrix(boost_cf)
```


### Random Forests

```{r}
set.seed(1)
rand_f <- randomForest(diagnosis ~ ., data = Training_M, mtry=4, importance = T)
varImpPlot(rand_f, type=1,cex = 0.7) # we print out the variable importance plot too

rf_pred<- predict(rand_f, Validation_M, type="class")
rf_cf <- confusionMatrix(as.factor(rf_pred), Validation_M$diagnosis, positive = "1")
draw_confusion_matrix(rf_cf)
```


### ROC curves

```{r}

 # For the boosting
response_boost <- data.frame(Validation_M[,1], boost_pred$class) 
response_boost$Validation_M...1.<- as.numeric(as.character(response_boost[,1]))
response_boost$boost_pred.class<- as.numeric(as.character(response_boost[,2]))
roc_score_boost <- roc(data= response_boost , response=Validation_M...1., boost_pred.class) #AUC score


# For bagging
response_bag <- data.frame(Validation_M[,1], bag_pred$class) 
response_bag$Validation_M...1.<- as.numeric(as.character(response_bag[,1]))
response_bag$bag_pred.class<- as.numeric(as.character(response_bag[,2]))
roc_score_bag <- roc(data= response_bag , response=Validation_M...1., bag_pred.class) #AUC score


# For Random Forests
response_rf <- data.frame(Validation_M[,1], rf_pred) 
response_rf$Validation_M...1.<- as.numeric(as.character(response_rf[,1]))
response_rf$rf_pred<- as.numeric(as.character(response_rf[,2]))
roc_score_rf <- roc(data= response_rf , response=Validation_M...1., rf_pred) #AUC score

# Plot ROC curves side by side
par(mfrow=c(1,3)) 
plot(roc_score_boost ,main ="ROC curve for Boosting", cex= 1)
plot(roc_score_bag ,main ="ROC curve for Bagging", cex=1)
plot(roc_score_rf ,main ="ROC curve for Random Forests", cex=1)

```


### majority vote -> combined

### average of models -> combined

## Comparision of Models

# Unsupervised Learning

## Cluster Analysis

### K-Means Clustering


```{r K-Means Clustering Preprocess}

set.seed(1)

# Duplicate Original to a Cluster Dataframe
KClusteringDF <- ORIGINAL

# Make Sure to be as Dataframe
KClusteringDF <- data.frame(KClusteringDF)

# Remove the "ID" Variable
KClusteringDF <- KClusteringDF[,-1]

# Preprocess Data
Norm_Kmeans <- preProcess(KClusteringDF, method = c("center", "scale"))
KClusteringDF_Preprocess <- predict(Norm_Kmeans, KClusteringDF)

# Separate Benign and Malign into 2 Datasets
Benign_ClusterDF <- KClusteringDF_Preprocess[KClusteringDF_Preprocess$diagnosis == 0,]
Malign_ClusterDF <- KClusteringDF_Preprocess[KClusteringDF_Preprocess$diagnosis == 1,]

# Without Preprocess for Malign DF
Malign_ClusterDF_No_Scale <- KClusteringDF[KClusteringDF$diagnosis == 1,]

```

#### K-Means Clustering with all Dataset - Model 1

Let's check if accounting for the whole dataset, we can find meaningful clusters.

```{r K-means Clustering Model 1}

set.seed(1)

# Load Library
library(factoextra)

# Labeling Tumors Type as Row Name
KClusteringDF_Preprocess$diagnosis <- factor(KClusteringDF_Preprocess$diagnosis, levels = c(0,1), labels=c("Benign","Malign"))

rownames(KClusteringDF_Preprocess) <- paste(KClusteringDF_Preprocess$diagnosis, 1:dim(KClusteringDF_Preprocess)[1], sep = "_")

# Optimal Number of Clusters
fviz_nbclust(KClusteringDF_Preprocess[,-1], kmeans, method = "wss")
fviz_nbclust(KClusteringDF_Preprocess[,-1], kmeans, method = "silhouette")
fviz_nbclust(KClusteringDF_Preprocess[,-1], kmeans, method = "gap_stat")

# Create Clusters
Cluster_ALL <- kmeans(KClusteringDF_Preprocess[,-1], centers = 2, iter.max = 100, nstart = 100)

```

> Comments: We can find the optimal number of clusters with 3 differents methods: **WSS** ( Within-Cluster-Sum of Squared Errors) or also called **Elbow Method**, The **Silhouette Method** which accounts for the separation between clusters or lastly the **Gap Statistic**. Here all three methods give us the optimal number of 2 clusters, which make sense when taking into accounts the fact that there is either Benign or Malign type of tumors. Let's see it graphically and how it performs with the real word. 

##### Model 1 - Plot

```{r K-means Clustering Model 1 - Plot}

set.seed(1)

# Plotting Clusters of Model 1
fviz_cluster(Cluster_ALL, data = KClusteringDF_Preprocess[,-1], main="Cluster Model 1", labelsize = 0)+ geom_text(
    label=rownames(KClusteringDF_Preprocess), 
    nudge_x = 0.25, nudge_y = 0.25, 
    check_overlap = T, size=2)

```

> Comments: We can see that K-Means without any human intervention found 2 clusters to be optimal on the whole dataset, and separated benign and malign tumors accordingly. We could check how it performed. Here Cluster Number 1 would be the Malign Tumors and Cluser Number 2 the Benign Tumors.

##### Model 1 - Performance

```{r Model 1 Cluster Performance}

set.seed(1)

# Let's Convert some results to comparable clusters and values
DF_Cluster_Performance <- as.data.frame(Cluster_ALL$cluster)
DF_Cluster_Performance <- cbind(DF_Cluster_Performance, KClusteringDF_Preprocess$diagnosis)
rownames(DF_Cluster_Performance) <- c(1:dim(DF_Cluster_Performance))
DF_Cluster_Performance$`Cluster_ALL$cluster` <- ifelse(DF_Cluster_Performance$`Cluster_ALL$cluster` == 1, 1,0)
DF_Cluster_Performance$`KClusteringDF_Preprocess$diagnosis` <- ifelse(DF_Cluster_Performance$`KClusteringDF_Preprocess$diagnosis` == "Malign",1,0)

# Convert as Factor the Binary Outcomes
DF_Cluster_Performance$`Cluster_ALL$cluster` <- factor(DF_Cluster_Performance$`Cluster_ALL$cluster`)
DF_Cluster_Performance$`KClusteringDF_Preprocess$diagnosis` <- factor(DF_Cluster_Performance$`KClusteringDF_Preprocess$diagnosis`)

# Confusion Matrix
Confusion_Matrix_K_Means1 <- confusionMatrix(data = DF_Cluster_Performance$`Cluster_ALL$cluster`, reference = DF_Cluster_Performance$`KClusteringDF_Preprocess$diagnosis`,positive = "1")

# Create the Function for Confusion Matrix
draw_confusion_matrix_K_Means1 <- function(cm) {

  layout(matrix(c(1,1,2)))
  par(mar=c(2,2,2,2))
  plot(c(100, 345), c(300, 450), type = "n", xlab="", ylab="", xaxt='n', yaxt='n')
  title('CONFUSION MATRIX for K-Means - Model 1', cex.main=2)

  # create the matrix 
  rect(150, 430, 240, 370, col='#1c6155')
  text(195, 435, 'Benign', cex=1.2)
  rect(250, 430, 340, 370, col='#1c615570')
  text(295, 435, 'Malignant', cex=1.2)
  text(125, 370, 'Predicted', cex=1.3, srt=90, font=2)
  text(245, 450, 'Actual', cex=1.3, font=2)
  rect(150, 305, 240, 365, col='#1c615570')
  rect(250, 305, 340, 365, col='#1c6155')
  text(140, 400, 'Benign', cex=1.2, srt=90)
  text(140, 335, 'Malignant', cex=1.2, srt=90)

  # add in the cm results 
  res <- as.numeric(cm$table)
  text(195, 400, res[1], cex=1.6, font=2, col='white')
  text(195, 335, res[2], cex=1.6, font=2, col='white')
  text(295, 400, res[3], cex=1.6, font=2, col='white')
  text(295, 335, res[4], cex=1.6, font=2, col='white')

  # add in the specifics 
  plot(c(100, 0), c(100, 0), type = "n", xlab="", ylab="", main = "DETAILS", xaxt='n', yaxt='n')
  text(10, 85, names(cm$byClass[1]), cex=1.2, font=2)
  text(10, 70, round(as.numeric(cm$byClass[1]), 3), cex=1.2)
  text(30, 85, names(cm$byClass[2]), cex=1.2, font=2)
  text(30, 70, round(as.numeric(cm$byClass[2]), 3), cex=1.2)
  text(50, 85, names(cm$byClass[5]), cex=1.2, font=2)
  text(50, 70, round(as.numeric(cm$byClass[5]), 3), cex=1.2)
  text(70, 85, names(cm$byClass[6]), cex=1.2, font=2)
  text(70, 70, round(as.numeric(cm$byClass[6]), 3), cex=1.2)
  text(90, 85, names(cm$byClass[7]), cex=1.2, font=2)
  text(90, 70, round(as.numeric(cm$byClass[7]), 3), cex=1.2)

  # add in the accuracy information 
  text(30, 35, names(cm$overall[1]), cex=1.5, font=2)
  text(30, 20, round(as.numeric(cm$overall[1]), 3), cex=1.4)
  text(70, 35, names(cm$overall[2]), cex=1.5, font=2)
  text(70, 20, round(as.numeric(cm$overall[2]), 3), cex=1.4)
}

# Plot the Confusion Matrix
draw_confusion_matrix_K_Means1(Confusion_Matrix_K_Means1)


```


> Comments: We can appreciate the K-Means algorithm to have found 2 types of differents tumours in our dataset (Malign and Benign), showing that we do have good seperability from our features. The Accuracy is not quite good, again the clustering is not aimed at being good in predictions but rather show some insights about the dataset mixed with some knowledge in the field. 


#### K-Means Clustering with only Malign Tumors - Model 2

Since Cancerous Tumors in the Breasts are not all equal, some of them being at different stages or type, we could apply the K-Means Clustering Model to find out if there is some separations among them and if could suggest an priory number of cluster to further analysis for medical researches. 

[Types of Breast Cancer - American Society](https://www.cancer.org/cancer/breast-cancer/about/types-of-breast-cancer.html)

[Breast Cancer Stages](https://www.cancer.org/cancer/breast-cancer/understanding-a-breast-cancer-diagnosis/stages-of-breast-cancer.html)

For examples, we can find this article about how stages are rated:

"In both staging systems, 7 key pieces of information are used:

1. The extent (size) of the tumor (T): **How large is the cancer?**  Has it grown into nearby areas?
2. The spread to nearby lymph nodes (N): Has the cancer spread to nearby lymph nodes? If so, how many?
3. The spread (metastasis) to distant sites (M): Has the cancer spread to distant organs such as the lungs or liver?
4. Estrogen Receptor (ER) status: Does the cancer have the protein called an estrogen receptor?
5. Progesterone Receptor (PR) status: Does the cancer have the protein called a progesterone receptor?
6. HER2 status: Does the cancer make too much of a protein called HER2?
7. Grade of the cancer (G): How much do the cancer cells look like normal cells?"

We can see that we lack a lot of information only using this dataset, we could only infer the size of the cancer based on 1 tumor, without nearby information. Thus we will be limited in the clustering method to only size as an information for the stage of the tumor. 


```{r K-means Clustering Model 2}

set.seed(1)

# Load Library
library(factoextra)

# Labeling Tumors Type as Row Name
Malign_ClusterDF$diagnosis <- factor(Malign_ClusterDF$diagnosis)

# Optimal Number of Clusters
fviz_nbclust(Malign_ClusterDF[,-1], kmeans, method = "wss")
fviz_nbclust(Malign_ClusterDF[,-1], kmeans, method = "silhouette")
fviz_nbclust(Malign_ClusterDF[,-1], kmeans, method = "gap_stat")

# Create Clusters
Cluster_Malign_1 <- kmeans(Malign_ClusterDF[,-1], centers = 3, iter.max = 100, nstart = 100)
Cluster_Malign_2 <- kmeans(Malign_ClusterDF[,-1], centers = 2, iter.max = 100, nstart = 100)

```

> Comments: All 3 methods don't converge to the same number of clusters, but we can see that the Elbow Method and Silhouette Method would either say 2 or 3 groups being optimal. The Gap Statistic show no cluster separations from the Malign tumors. We can try separating into 2 and 3 clusters and see the profiling of those groups. (For simplictiy, we only compare mean variables as meaningful measures to intepret our tumors.)


##### Model 2 with 3 Clusters - Plot

<center>

```{r K-means Clustering Model 2 - Plot}

set.seed(1)

# Plotting Clusters of Model 1
fviz_cluster(Cluster_Malign_1, data = Malign_ClusterDF[,-1], main="Cluster Model 2 - Only Malign Tumors", subtitle="with 3 Clusters", labelsize = 0)

```

</center>

> Comments: Some overlap occurs in this 2D graphs, but considering all dimensions, there is no overlap at all. We can see that we could with human interpretation, see that there would be indeed 3 different clusters in the Malign Tumors. Let's check the centroid. 

#####  Centroid of Model 2 with 3 Clusters

```{r Model 2 Centroid - 3 Clusters}

Clusters_Malign_1_Centers <- Cluster_Malign_1$centers

DT::datatable(round(Clusters_Malign_1_Centers,5), caption = "Centroid from Model 2 - 3 Clusters")


```

> Comments: We can see that the **radius_mean** (**perimeter_mean** and **area_mean** are quite similar) is indeed being one variable cleary seperating the Malign Tumors into 3 clusters, **concavity_mean** and **compactness_mean** as well. **smoothness_mean** is also seperating clusters from each other. (For simplictiy, we only compare mean variables as meaningful measures to intepret our tumors.)


#####  Cluster Members with 3 Clusters 

```{r Cluster Member for 3 Clusters in Malign}

# Cluster Members
split1<- split(Malign_ClusterDF_No_Scale, Cluster_Malign_1$cluster)

# Split Cluster to Original Malign Not Scaled DF
cluster_1 <- split1$`1`
cluster_2 <- split1$`2`
cluster_3 <- split1$`3`

# Data Table
DT::datatable(cluster_1[,-1], caption = "Cluster 1")
DT::datatable(cluster_2[,-1], caption = "Cluster 2 ")
DT::datatable(cluster_3[,-1], caption = "Cluster 3")

```


#####  Boxplots for Comparison of **radius_mean**

```{r boxplot comparison of 3 clusters}

# Load Libaries
library(ggpubr)
library(ggplot2)

# Boxplot with ggplot
boxcluster1 <- ggplot(cluster_1) +
 aes(x = "", y = radius_mean) +
 geom_boxplot(fill = "#1c6155") +
 labs(title = "Cluster 1", 
 subtitle = "in millimiters") +
 theme_minimal() + ylim(10, 30)

boxcluster2 <- ggplot(cluster_2) +
 aes(x = "", y = radius_mean) +
 geom_boxplot(fill = "#1c6155") +
 labs(title = "Cluster 2", 
 subtitle = "in millimiters") +
 theme_minimal() + ylim(10, 30)

boxcluster3 <- ggplot(cluster_3) +
 aes(x = "", y = radius_mean) +
 geom_boxplot(fill = "#1c6155") +
 labs(title = "Cluster 3", 
 subtitle = "in millimiters") +
 theme_minimal() + ylim(10, 30)

ggarrange1 <- ggarrange(boxcluster1,boxcluster2,boxcluster3, ncol = 3)
annotate_figure(ggarrange1,
                top = text_grob("Boxplot for radius_mean Among Clusters", color = "black", face = "bold", size = 14))

```

> Comments: We can see some tumors being greater than 20mm or between 15mm and 20mm, and lastly under 15mm. Such result in the clusters median is very intersting and knowing how the staging system is done can actually lead us to prefer the clustering into 2 groups: 1 cluster being below 20mm and the other greater or equal than 20mm. We will do as such in the following part.

#####  Model 2 with 2 Clusters - Plot

<center>

```{r K-means Clustering Model 4 - Plot}

set.seed(1)

# Plotting Clusters of Model 1
fviz_cluster(Cluster_Malign_2, data = Malign_ClusterDF[,-1], main="Cluster Model 2 - Only Malign Tumors", subtitle="with 2 Clusters", labelsize = 0)

```

</center>

> Comments: Overlapping still occurs in such 2d graphs, but we can see also the trend of 2 groups, the Red one being more spread than the blue one, and some spread also happen in the bottom center of the plot for the blue cluster. 

#####  Centroid of Model 2 with 2 Clusters

```{r Model 2 Centroid - 2 Clusters}

Clusters_Malign_2_Centers <- Cluster_Malign_2$centers

DT::datatable(round(Clusters_Malign_2_Centers,5), caption = "Centroid from Model 2 - 2 Clusters")

```

> Comments: We can also see the **radius_mean** (**perimeter_mean** and **area_mean**) being very important in the separation, **compactness_mean** and **concavity_mean** as well. **symmetry_mean** is also quite different and **smoothness_mean** as well. 

#####  Cluster Members with 2 Clusters 

```{r Cluster Member for 2 Clusters in Malign}

# Cluster Members
split2<- split(Malign_ClusterDF_No_Scale, Cluster_Malign_2$cluster)

# Split Cluster to Original Malign Not Scaled DF
cluster2_1 <- split2$`1`
cluster2_2 <- split2$`2`

# Data Table
DT::datatable(cluster2_1[,-1], caption = "Cluster 1")
DT::datatable(cluster2_2[,-1], caption = "Cluster 2")

```


#####  Boxplots for Comparison of **radius_mean**

```{r boxplot comparison of 2 clusters}

# Load Libaries
library(ggpubr)
library(ggplot2)

# Boxplot with ggplot
boxcluster1 <- ggplot(cluster_1) +
 aes(x = "", y = radius_mean) +
 geom_boxplot(fill = "#1c6155") +
 labs(title = "Cluster 1 (T2)", 
 subtitle = "in millimiters") +
 theme_minimal() + ylim(10, 30)

boxcluster2 <- ggplot(cluster_2) +
 aes(x = "", y = radius_mean) +
 geom_boxplot(fill = "#1c6155") +
 labs(title = "Cluster 2 (T1)", 
 subtitle = "in millimiters") +
 theme_minimal() + ylim(10, 30)

ggarrange2 <- ggarrange(boxcluster1,boxcluster2)
annotate_figure(ggarrange2,
                top = text_grob("Boxplot for radius_mean Among Clusters", color = "black", face = "bold", size = 14))

```

> Comments: without having the full key pieces information for the staging systems from the American Cancer Society, we can already have some metrics for the **T** key which is the size of the tumor, but without the nearby areas. The dataset suggest that the measure are for primary tumors only. If we look at the **Cluster 1**, the **radius_mean** median seems to be higher than 2cm or 20mm but less than 5cm or 50mm. Thus we would attribute the **T2** key to this Cluster. **Cluster 2** in opposite is having an median close to 1.4cm or 14mm, since this is less than 2cm ro 20mm, we could attribute the key **T1** to this cluster. Nevertheless, we should remember that some member of Cluster 1 are less than 20mm, and thus we shouldn't categorize them as **T2** following the guidelines. For simplicity, we will keep those tumors in the Cluster 1 but if we wanted to decide or not wether a member is subject to **T2**, we should use other metrics to check the size exactitude before removing it to the **T2** label. 

Cluster 2 with T1 could potential lead us to such Stages:

> **Stage IA**: The tumor is small, invasive, and has not spread to the lymph nodes (**T1**, N0, M0). **Stage IB**: Cancer has spread to the lymph nodes and the cancer in the lymph node is larger than 0.2 mm but less than 2 mm in size. There is either no evidence of a tumor in the breast or the tumor in the breast is 20 mm or smaller (T0 or **T1**, N1mi, M0). **Stage IIIC**: A tumor of any size that has spread to 10 or more axillary lymph nodes, the internal mammary lymph nodes, and/or the lymph nodes under the collarbone. It has not spread to other parts of the body (any T, N3, M0). **Stage IV** (metastatic): The tumor can be any size and has spread to other organs, such as the bones, lungs, brain, liver, distant lymph nodes, or chest wall (any T, any N, M1). Metastatic cancer found when the cancer is first diagnosed occurs about 6% of the time. This may be called de novo metastatic breast cancer. Most commonly, metastatic breast cancer is found after a previous diagnosis of early stage breast cancer.

Cluster 1 with **T2** could potential lead us to such Stages:

> **Stage IIA**: Any 1 of these conditions: The tumor is larger than 20 mm but not larger than 50 mm and has not spread to the axillary lymph nodes (**T2**, N0, M0). **Stage IIB**: The tumor is larger than 20 mm but not larger than 50 mm and has spread to 1 to 3 axillary lymph nodes (**T2**, N1, M0). **Stage IIIA**: The tumor of any size has spread to 4 to 9 axillary lymph nodes or to internal mammary lymph nodes. It has not spread to other parts of the body (T0, T1, **T2**, or T3; N2; M0). **Stage IIIC**: A tumor of any size that has spread to 10 or more axillary lymph nodes, the internal mammary lymph nodes, and/or the lymph nodes under the collarbone. It has not spread to other parts of the body (any T, N3, M0). **Stage IV** (metastatic): The tumor can be any size and has spread to other organs, such as the bones, lungs, brain, liver, distant lymph nodes, or chest wall (any T, any N, M1). Metastatic cancer found when the cancer is first diagnosed occurs about 6% of the time. This may be called de novo metastatic breast cancer. Most commonly, metastatic breast cancer is found after a previous diagnosis of early stage breast cancer. 

> Comments: We can see that we lack a lot of information to actually get to the actual stage of the cancerous breast tumors, depending on the source, we may lack 2 more information if we follow *Cancer.Net* staging system: **Node** (N - Has the tumor spread to the lymph nodes? If so, where, what size, and how many?) or **Metastasis** (M - Has the cancer spread to other parts of the body?). *The American Cancer Society* requires way more information, up to 7 in total plus additional recurrence test. Here is the 7 keys parameters: 

[image](7 Keys.png)

[Breast Cancer Stages - cancer.org](https://www.cancer.org/cancer/breast-cancer/understanding-a-breast-cancer-diagnosis/stages-of-breast-cancer.html)

[Breast Cancer: Stages - Cancer.Net](https://www.cancer.net/cancer-types/breast-cancer/stages)


#####  Proportions of T1 and T2 - Pie Chart

```{r Staging Proportions of T1 and T2}

# Computing Proportions of T1 and T2
Proportions_T1 <- nrow(cluster2_2)/nrow(Malign_ClusterDF_No_Scale)
Proportions_T2 <- nrow(cluster2_1)/nrow(Malign_ClusterDF_No_Scale)

# Rounding Proportions
Proportions_T1 <- round(Proportions_T1,3)
Proportions_T2 <- round(Proportions_T2,3)

# Pie Chart Dataframe
Pie_T1_T2 <- data.frame(
  t = c("T1", "T2"),
  n = c(151, 61),
  prop = c(Proportions_T1, Proportions_T2))

# Pie ggplot
ggplot(Pie_T1_T2, aes(x="", y=n, fill=t)) +
  geom_bar(stat="identity", width=1) +
  coord_polar("y", start=0) + theme_void() + geom_text(aes(label = paste0(100*prop, "%")), position = position_stack(vjust=0.5), color="white", size=6) +
  labs(x = NULL, y = NULL, fill = "T Category") + scale_fill_manual(values=c("#1c6155","#66807b")) + ggtitle("Pie Chart of T Category Proportions for Malign Tumors (221 obs.)")


```



> Comments:


# Conclusion

# References 

[Logistic Regression in Machine Learning](https://www.javatpoint.com/logistic-regression-in-machine-learning)

[Convergence Error in Logistic Regression](https://stackoverflow.com/questions/8596160/why-am-i-getting-algorithm-did-not-converge-and-fitted-prob-numerically-0-or)

[Penalized Logistic Regression Essentials in R: Ridge, Lasso and Elastic Net](http://www.sthda.com/english/articles/36-classification-methods-essentials/149-penalized-logistic-regression-essentials-in-r-ridge-lasso-and-elastic-net/)

[Lasso Regression in R (Step-by-Step)](https://www.statology.org/lasso-regression-in-r/)

[ROC Curve](https://www.digitalocean.com/community/tutorials/plot-roc-curve-r-programming)

[How to create a ROC curve in R](https://www.datatechnotes.com/2019/03/how-to-create-roc-curve-in-r.html)

[VIF Inflation Error](https://www.statology.org/variance-inflation-factor-r/)

[Neural Network Models in R](https://www.datacamp.com/tutorial/neural-network-models-r)

[How to choose the number of hidden layers and nodes in a feedforward neural network?](https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw)

[Introduction to Neural Networks for Java (second edition) by Jeff Heaton - Google Books](https://books.google.it/books?id=Swlcw7M4uD8C&lpg=PA158&dq=Introduction%20to%20Neural%20Networks%20for%20Java%2C%20Second%20Edition%20The%20Number%20of%20Hidden%20Layers&hl=it&pg=PA158#v=onepage&q=Introduction%20to%20Neural%20Networks%20for%20Java,%20Second%20Edition%20The%20Number%20of%20Hidden%20Layers&f=false)

[Cluster Analysis in R](https://www.r-bloggers.com/2021/04/cluster-analysis-in-r/)

[Do we need to set training set and testing set for clustering?](https://stats.stackexchange.com/questions/268934/do-we-need-to-set-training-set-and-testing-set-for-clustering)

[K-means Clustering: Algorithm, Applications, Evaluation Methods, and Drawbacks](https://towardsdatascience.com/k-means-clustering-algorithm-applications-evaluation-methods-and-drawbacks-aa03e644b48a)

[Types of Breast Cancer - American Society](https://www.cancer.org/cancer/breast-cancer/about/types-of-breast-cancer.html)

[Breast Cancer Stages - cancer.org](https://www.cancer.org/cancer/breast-cancer/understanding-a-breast-cancer-diagnosis/stages-of-breast-cancer.html)

[Breast Cancer: Stages - Cancer.Net](https://www.cancer.net/cancer-types/breast-cancer/stages)


