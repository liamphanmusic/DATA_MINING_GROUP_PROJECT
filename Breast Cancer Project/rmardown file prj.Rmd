---
title: "Breast Cancer Project"
author: "Manuela Giansante, Liam Phan, MIcheal Bingler"
date: "2022-12-01"
output: html_document
---

```{r setup, include=FALSE, set.seed(1)}

knitr::opts_chunk$set(echo = TRUE,
                      cache = TRUE)

rm(list = ls())

```

```{r Libraries and functions, message=FALSE, warning= FALSE}

library(data.table)
library(summarytools)
library(ggplot2)
library(data.table) # fread
library(rbokeh) # visualization of data
library(summarytools) # ORIGINALSummary
library(adabag) # bagging and boosting
library(caret) # pre-processing
library(dplyr) # select, mutate_if
library(fastDummies) # dummy_cols
library(splitTools) # data partition
library(rpart) # classification tree
library(rpart.plot) # plot regression trees
library(DT) # datatable
library(corrplot) # corrplot
library(gains) # gain
library(randomForest) # randomForest
library(cluster) # hierarchical clustering
library(knitr) # kable
library(kableExtra) # kbl
library(ggplot2) # ggplot
library(MASS) # lda, qda, etc.
library(dplyr)
library(klaR) # partimat
library(forecast)
library(pROC)


```

```{r}

# Confusion matrix
draw_confusion_matrix <- function(cm) {
  layout(matrix(c(1,1,2)))
  par(mar=c(2,2,2,2))
  plot(c(100, 345), c(300, 450), type = "n", xlab="", ylab="", xaxt='n', yaxt='n')
  title('CONFUSION MATRIX', cex.main=2)

  # create the matrix 
  rect(150, 430, 240, 370, col='#3F97D0')
  text(195, 435, 'No', cex=1.2)
  rect(250, 430, 340, 370, col='#F7AD50')
  text(295, 435, 'Yes', cex=1.2)
  text(125, 370, 'Predicted', cex=1.3, srt=90, font=2)
  text(245, 450, 'Actual', cex=1.3, font=2)
  rect(150, 305, 240, 365, col='#F7AD50')
  rect(250, 305, 340, 365, col='#3F97D0')
  text(140, 400, 'No', cex=1.2, srt=90)
  text(140, 335, 'Yes', cex=1.2, srt=90)

  # add in the cm results 
  res <- as.numeric(cm$table)
  text(195, 400, res[1], cex=1.6, font=2, col='white')
  text(195, 335, res[2], cex=1.6, font=2, col='white')
  text(295, 400, res[3], cex=1.6, font=2, col='white')
  text(295, 335, res[4], cex=1.6, font=2, col='white')

  # add in the specifics 
  plot(c(100, 0), c(100, 0), type = "n", xlab="", ylab="", main = "DETAILS", xaxt='n', yaxt='n')
  text(5, 85, names(cm$byClass[1]), cex=1.2, font=2)
  text(5, 70, round(as.numeric(cm$byClass[1]), 3), cex=1.2)
  text(23, 85, names(cm$byClass[2]), cex=1.2, font=2)
  text(23, 70, round(as.numeric(cm$byClass[2]), 3), cex=1.2)
  text(41, 85, names(cm$byClass[5]), cex=1.2, font=2)
  text(41, 70, round(as.numeric(cm$byClass[5]), 3), cex=1.2)
  text(59, 85, names(cm$byClass[6]), cex=1.2, font=2)
  text(59, 70, round(as.numeric(cm$byClass[6]), 3), cex=1.2)
  text(77, 85, names(cm$byClass[7]), cex=1.2, font=2)
  text(77, 70, round(as.numeric(cm$byClass[7]), 3), cex=1.2)
  text(95, 85, names(cm$byClass[8]), cex=1.2, font=2)
  text(95, 70, round(as.numeric(cm$byClass[8]), 3), cex=1.2)

  # add in the accuracy information 
  text(30, 35, names(cm$overall[1]), cex=1.5, font=2)
  text(30, 20, round(as.numeric(cm$overall[1]), 3), cex=1.4)
  text(70, 35, names(cm$overall[2]), cex=1.5, font=2)
  text(70, 20, round(as.numeric(cm$overall[2]), 3), cex=1.4)
}  

```

# Data Analysis

## Structure of Data

```{r Data reading}

ORIGINAL <- fread("data/Breast_Cancer/breast-cancer.csv")


# printing sumamry
print(dfSummary(ORIGINAL, valid.col = FALSE, graph.magnif = 0.75, plain.ascii = FALSE, html = TRUE, style ='grid', silent = TRUE), max.tbl.height = 300, width = 80, method = "render")

```

## Missing values

## Distribution of Data

```{r boxplots}

norm.value <- preProcess(ORIGINAL, method = c("center", "scale"))
ORIGINAL.boxplot <- predict(norm.value, ORIGINAL)
ORIGINAL.boxplot <-  melt(select(ORIGINAL.boxplot, -c(id)))

library(ggplot2)
ggplot(ORIGINAL.boxplot, aes(x = diagnosis, y = value)) +
  facet_wrap(~variable) + 
  stat_boxplot(geom ='errorbar') + 
  geom_boxplot()

```

## Correlation

```{r corrplot, fig.width=15,fig.height=15}

# select numeric variables
ORIGINAL_is.numeric <- ORIGINAL %>% 
  select_if(is.numeric)

ORIGINAL_heatmap <- round(cor(ORIGINAL_is.numeric, use = "complete.obs"), 1)
ORIGINAL_heatmap <- melt(ORIGINAL_heatmap)

#  corrplot
ggplot(ORIGINAL_heatmap, aes(x=Var1, y=Var2, fill=value)) +
  geom_tile()+
  scale_fill_gradient(low = "pink", high = "dark red")+
  labs(title="Correlation Heatmap",x="", y="")+
  geom_text(aes(x=Var1, y=Var2, label=value), color="white")+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1), plot.title = element_text(hjust=0.5))

```

# Data Preparation

## General 

## Transformation

```{r Transform Outcome Variable to Binary Factor Variable}

# Transform Character Format to Binary Numerical Values on Outcome Variable "Diagnosis"

ORIGINAL[diagnosis == "M", c("diagnosis")] <- 1  # 1 for Malign Outcome
ORIGINAL[diagnosis == "B", c("diagnosis")] <- 0  # 0 for Benign Outcome 

ORIGINAL$diagnosis <- as.factor(ORIGINAL$diagnosis) # To Factor Variable 

```


## Standardization

ORIGINAL
ORIGINAL center scale
ORIGINAL range

## Partitioning 

```{r Partitioning the Dataset}

splitting <- sample(1:3,size=nrow(ORIGINAL),replace=TRUE,prob=c(0.5,0.3,0.2))
Training <- ORIGINAL[splitting==1,]
Validation <- ORIGINAL[splitting==2,]
Test <- ORIGINAL[splitting==3,]

# Checking if proportions are right
Prop_Training <- (nrow(Training)/nrow(ORIGINAL))*100
Prop_Validation <- (nrow(Validation)/nrow(ORIGINAL))*100
Prop_Test <- (nrow(Test)/nrow(ORIGINAL))*100

# Print Proportion
paste("The Proportions are:", round(Prop_Training,2),"% In Training,",round(Prop_Validation,2),"% In Validation, and ",round(Prop_Test,2),"% In Test")

```

TRAINING 50, val 30, TEST 20

# Supervised Learning

## Logistic Regression

Assumptions for Logistic Regression: 

  1. The dependent variable must be categorical in nature.
  2. The independent variable should not have multi-collinearity.
  
Type of Logistic Regression: 

  1. **Binomial** (we will use this type since we only have binary outcomes, malign or benign)
  2. Multinational 
  3. Ordinal 

### Fit the Logistic Regression Model 

```{r Dataset Duplicata}

# Duplicate the Training and Validation Set
Training_Logistic <- Training
Validation_Logistic <- Validation

# Remove the "ID" Variable
Training_Logistic <- Training_Logistic[,-c("id")]
Validation_Logistic <- Validation_Logistic[,-c("id")]

```

```{r Fit Logistic Regression}

# Fit The Logistic Regression Model
Logistic_Model_1 <- glm(diagnosis ~ radius_mean + texture_mean + perimeter_mean + area_mean + smoothness_mean + compactness_mean + concavity_mean + `concave points_mean` + symmetry_mean + fractal_dimension_mean + radius_se + texture_se + perimeter_se + area_se + smoothness_se + compactness_se + concavity_se + `concave points_se` + symmetry_se + fractal_dimension_se, family=binomial(link='logit'), data=Training_Logistic)

# Disable Scientific Notation
options(scipen=999)

# Model Summary
summary(Logistic_Model_1)
```

> Comments: Trying to fit every variable in our logistic regression showed an convergence error when there is complete separation, where 1 group is completely composed of 0s or 1s. To deal with this we should use a penalized model because of too many variables included in our model compared to the number of observations. (See **[Convergence Error in Logistic Regression]** and **[Penalized Logistic Regression Essentials in R: Ridge, Lasso and Elastic Net]** in References) 

### Fit a Penalized Model for Logistic Regression

There is 3 differents methods when it comes to Penalized Logistic Regression Model:

  1. **ridge regression**: variables with minor contribution have their coefficients close to zero. However, all the variables are incorporated in the model. This is useful when all variables need to be incorporated in the model according to domain knowledge.
  2. **lasso regression**: the coefficients of some less contributive variables are forced to be exactly zero. Only the most significant variables are kept in the final model.
  3. **elastic net regression**: the combination of ridge and lasso regression. It shrinks some coefficients toward zero (like ridge regression) and set some coefficients to exactly zero (like lasso regression)
  
For this case, we will use a **Lasso Regression Model**

```{r Fit Penalized Logistic Regression}

# Required Packages
library(tidyverse)
library(caret)
library(glmnet)

# Setting Seed
set.seed(1)

#define response variable
y_lasso <- as.numeric(Training_Logistic$diagnosis)

#define matrix of predictor variables
x_lasso <- data.matrix(Training_Logistic[,-c("diagnosis")])

#perform k-fold cross-validation to find optimal lambda value - alpha = 1 is for using Lasso Method
cv_model <- cv.glmnet(x_lasso, y_lasso, alpha = 1)

#find optimal lambda value that minimizes test MSE
best_lambda <- cv_model$lambda.min
best_lambda

#produce plot of test MSE by lambda value
plot(cv_model) 

```


> Comments: 



## Classification Tree



```{r}
set.seed(1)
# data.preparation
# so it works for me, delete later(?)
ORIGINAL_M<- data.frame(ORIGINAL) # remember to delete later
# Transform Character Format to Binary Numerical Values on Outcome Variable "Diagnosis"

ORIGINAL_M$diagnosis <- ifelse(ORIGINAL_M$diagnosis == "M", 1, 0)
ORIGINAL_M$diagnosis <- as.factor(ORIGINAL_M$diagnosis) # To Factor Variable 
ORIGINAL_M2<- ORIGINAL_M[,-c(1)]


splitting <- sample(1:3,size=nrow(ORIGINAL_M2),replace=TRUE,prob=c(0.5,0.3,0.2))
Training <- ORIGINAL_M2[splitting==1,]
Validation <- ORIGINAL_M2[splitting==2,]
Test <- ORIGINAL_M2[splitting==3,]

# Checking if proportions are right
Prop_Training <- (nrow(Training)/nrow(ORIGINAL_M2))*100
Prop_Validation <- (nrow(Validation)/nrow(ORIGINAL_M2))*100
Prop_Test <- (nrow(Test)/nrow(ORIGINAL_M2))*100

# Print Proportion
paste("The Proportions are:", round(Prop_Training,2),"% In Training,",round(Prop_Validation,2),"% In Validation, and ",round(Prop_Test,2),"% In Test")
```

The outcome variable is a binary factor, we model a classification tree.
We first run a deep tree, with all the features included.
Then proceed to reduce the size of the deeper tree through pruning.


### run tree: 
```{r, fig.width=5,fig.height=5}

set.seed(1)
options(scipen=999)

tree_full <- rpart(diagnosis ~ ., 
              data = Training, 
              method = "class",  # "class" because Y is a binary factor
              minbucket = 1,
              cp = 0.00001) 

# Plot tree
rpart.plot(tree_full, yesno = TRUE, digits =-6)
length(tree_full$frame$var[tree_full$frame$var == "<leaf>"]) # End nodes

relevance<-as.data.frame(tree_full$variable.importance) #we get the ranking of the variables by importance
kable(relevance, row.names = T,col.names="Variable Importance")%>% kable_paper("hover", full_width = T) #built table


printcp(tree_full, digits = 6) # print complexity value
plotcp(tree_full, upper = "splits") # we plot the progression of complexity values

#Prune the tree
min_xerr<- which.min(tree_full$cptable[,"xerror"]) # select minimum cross-validation error
cp_bp <- tree_full$cptable[min_xerr,"CP"] # find the corresponding CP value, to get the "best pruned " tree


pruned_tree<- prune(tree_full, cp = cp_bp) # re-compute the tree with the selected Cp
rpart.plot(pruned_tree, yesno = TRUE, digits =-3)
length(pruned_tree$frame$var[pruned_tree$frame$var == "<leaf>"]) # how many end nodes
```

The fully grown tree is quite reduced in size, we still pruned the tree.

### Performance of Best Pruned Tree

```{r}
# classification prediction over validation data
pruned_pred <- predict(pruned_tree, Validation, type = "class")
pruned_prob <- predict(pruned_tree, Validation, type = "prob") # probabilities of belonging to 1


# confusion matrix and accuracy of classification tree
tree_cf<- confusionMatrix(pruned_pred, Validation$diagnosis, positive = "1")
draw_confusion_matrix(tree_cf)

```

Sensitivity lower than Specificity ( Malign diagnosis is minority). Accuracy pretty high.

```{r}
#ROC curve
response <- data.frame(Validation[,1], pruned_pred) 
response$Validation...1.<- as.numeric(as.character(response$Validation...1.))
response$pruned_pred<- as.numeric(as.character(response$pruned_pred))
roc_score <- roc(data= response , response=Validation...1., pruned_pred) #AUC score
plot(roc_score ,main ="ROC curve")


```


## K-Nearest Neighbor

```{r data prep knn}

ORIGINAL.KNN <- dplyr::select(ORIGINAL, -c("id"))
ORIGINAL.KNN$diagnosis <- factor(ifelse(ORIGINAL$diagnosis == "M", 1, 0))

# partition
inds <- partition(ORIGINAL.KNN$diagnosis, p = c(train = 0.5, valid = 0.3, test = 0.2))
ORIGINAL.KNN.train <- ORIGINAL.KNN[inds$train]
ORIGINAL.KNN.valid <- ORIGINAL.KNN[inds$valid]
ORIGINAL.KNN.test <- ORIGINAL.KNN[inds$test]

# standardize
norm.value <- preProcess(ORIGINAL.KNN.train, method = c("center", "scale"))
ORIGINAL.KNN.train <- predict(norm.value, ORIGINAL.KNN.train)
ORIGINAL.KNN.valid <- predict(norm.value, ORIGINAL.KNN.valid)
ORIGINAL.KNN.test <- predict(norm.value, ORIGINAL.KNN.test)

```

best k 

```{r knn best k}
accuracy.df <- data.frame(k = seq(1, 30, 1), accuracy = rep(0, 30))

# iterating over different k's
for(i in 1:30){
  # nearest neighbor
  KNN1 <- knn3(y = ORIGINAL.KNN.train$diagnosis, x = dplyr::select(ORIGINAL.KNN.train, -c(diagnosis)), k = i)

  # predictions response 
  KNN1.pred.valid.resp <- predict(KNN1, dplyr::select(ORIGINAL.KNN.valid, -c(diagnosis)), type = "class")
  
  # predictions prob 
  KNN1.pred.valid.prob <- predict(KNN1, dplyr::select(ORIGINAL.KNN.valid, -c(diagnosis)), type = "prob")[,2]
  
  # Confusionmatrix
  accuracy.df[i, 2] <- confusionMatrix(KNN1.pred.valid.resp, ORIGINAL.KNN.valid$diagnosis, positive = "1")$overall[1]
}

# table in markdown
datatable(accuracy.df)

# plot the k's 
ggplot(accuracy.df) +
 aes(x = k, y = accuracy) +
 geom_line(size = 0.7, colour = "#112646") +
 labs(x = "Number of k nearest neighbours", 
 y = "Accuracy", title = "Accuracy regarding k") +
 theme_minimal()
```

calculate best knn

```{r best knn}
# nearest neighbor
KNN1 <- knn3(y = ORIGINAL.KNN.train$diagnosis, x = dplyr::select(ORIGINAL.KNN.train, -c(diagnosis)), k = 3)

# predictions response 
KNN1.pred.valid.resp <- predict(KNN1, dplyr::select(ORIGINAL.KNN.valid, -c(diagnosis)), type = "class")

# predictions prob 
KNN1.pred.valid.prob <- predict(KNN1, dplyr::select(ORIGINAL.KNN.valid, -c(diagnosis)), type = "prob")[,2]

# Confusionmatrix
KNN1.conf.mat <- confusionMatrix(KNN1.pred.valid.resp, ORIGINAL.KNN.valid$diagnosis, positive = "1")
draw_confusion_matrix(KNN1.conf.mat)
```

## Neural Networks

## Discriminant Analysis

First I just run a discriminant analysis.

First data prep before model -> center / scale, 

```{r DA data prep}

ORIGINAL.DA <- dplyr::select(ORIGINAL, -c("id"))
ORIGINAL.DA$diagnosis <- factor(ifelse(ORIGINAL$diagnosis == "M", 1, 0))

# partition
inds <- partition(ORIGINAL.DA$diagnosis, p = c(train = 0.5, valid = 0.3, test = 0.2))
ORIGINAL.DA.train <- ORIGINAL.DA[inds$train]
ORIGINAL.DA.valid <- ORIGINAL.DA[inds$valid]
ORIGINAL.DA.test <- ORIGINAL.DA[inds$test]

# standardize
norm.value <- preProcess(ORIGINAL.DA.train, method = c("center", "scale"))
ORIGINAL.DA.train <- predict(norm.value, ORIGINAL.DA.train)
ORIGINAL.DA.valid <- predict(norm.value, ORIGINAL.DA.valid)
ORIGINAL.DA.test <- predict(norm.value, ORIGINAL.DA.test)

```

try with lda

```{r first DA}

# Fit the model
DA1 <- lda(diagnosis~., data = ORIGINAL.DA.train)

# Make predictions
predictions <- predict(DA1, ORIGINAL.DA.valid)

# predictions prob
DA1.pred.valid.prob <- predictions$posterior[,2]

# predictions response 
DA1.pred.valid.resp <- factor(predictions$class)

# confusion matrix
DA1.conf.mat <- confusionMatrix(DA1.pred.valid.resp, ORIGINAL.DA.valid$diagnosis, positive = "1")
draw_confusion_matrix(DA1.conf.mat)

# Lift chart
actual <- as.numeric(as.character(ORIGINAL.DA.valid$diagnosis))
prob <- as.numeric(DA1.pred.valid.prob)
DA1.gain <- gains(actual, prob)
barplot(DA1.gain$mean.resp / mean(actual), names.arg = DA1.gain$depth, xlab = "Percentile",
ylab = "Mean Response", main = "Decile-wise lift chart")

# Evaluating LDA
# doesn't work
# klaR::partimat(diagnosis~., data = ORIGINAL.DA.train, method="lda", plot.matrix = FALSE)

```

If bad -> Correlation in data -> qda continuing with it. If not continue with lda

```{r qda}

# Fit the model
DA2 <- qda(diagnosis~., data = ORIGINAL.DA.train)

# Make predictions
predictions <- predict(DA2, ORIGINAL.DA.valid)

# predictions prob
DA2.pred.valid.prob <- predictions$posterior[,2]

# predictions response 
DA2.pred.valid.resp <- factor(predictions$class)

# confusion matrix
DA2.conf.mat <- confusionMatrix(DA2.pred.valid.resp, ORIGINAL.DA.valid$diagnosis, positive = "1")
draw_confusion_matrix(DA2.conf.mat)

# Lift chart
actual <- as.numeric(as.character(ORIGINAL.DA.valid$diagnosis))
prob <- as.numeric(DA2.pred.valid.prob)
DA2.gain <- gains(actual, prob)
barplot(DA2.gain$mean.resp / mean(actual), names.arg = DA2.gain$depth, xlab = "Percentile",
ylab = "Mean Response", main = "Decile-wise lift chart")

```

Doesn't get better... lets continue with lda

We see different proportions -> need to refit model with cost to proportion

```{r lda different proportions}

# Fit the model
DA3 <- lda(diagnosis~., data = ORIGINAL.DA.train)

# Make predictions
predictions <- predict(DA2, ORIGINAL.DA.valid)

# predictions prob
DA2.pred.valid.prob <- predictions$posterior[,2]

# predictions response 
DA2.pred.valid.resp <- factor(predictions$class)

# confusion matrix
DA2.conf.mat <- confusionMatrix(DA2.pred.valid.resp, ORIGINAL.DA.valid$diagnosis, positive = "1")
draw_confusion_matrix(DA2.conf.mat)

# Lift chart
actual <- as.numeric(as.character(ORIGINAL.DA.valid$diagnosis))
prob <- as.numeric(DA2.pred.valid.prob)
DA2.gain <- gains(actual, prob)
barplot(DA2.gain$mean.resp / mean(actual), names.arg = DA2.gain$depth, xlab = "Percentile",
ylab = "Mean Response", main = "Decile-wise lift chart")

```

we know missclassification bad -> cost for misclassification model

3rd model

We see that outliers make it bad -> remove some rows

4th model

Some predictors unimportant -> remove some

5th model

```{r data prep for DA}

# remove outliers
# use a different filter than boxplot?
# https://statsandr.com/blog/outliers-detection-in-r/

ORIGINAL.DA <- select(ORIGINAL.DA, -c(id))



# normality -> log transform

# correlation between groups

```


```{r base model}

```

## Ensemble Methods

### bagging

```{r}
set.seed(1)
bagging<- bagging(diagnosis ~ ., data =Training)
```


### boosting

```{r}
set.seed(1)
boosting <- boosting(diagnosis ~ ., data = Training)
```


### rf

```{r}
set.seed(1)
rand_f <- randomForest(diagnosis ~ ., data = Training, mtry=4, importance = T)
varImpPlot(rand_f, type=1) # we print out the variable importance plot too
# print it better
```


### majority vote -> combined

### average of models -> combined

## Comparision of Models

# Unsupervised Learning

## Cluster Analysis

# Conclusion

```{r Data reading}

ORIGINAL <- fread("data/Breast_Cancer/breast-cancer.csv")

# printing sumamry
print(ORIGINALSummary(ORIGINAL, valid.col = FALSE, graph.magnif = 0.75, plain.ascii = FALSE, html = TRUE, style ='grid', silent = TRUE), max.tbl.height = 300, width = 80, method = "render")

```

```{r corrplot, fig.width=19,fig.height=19}
# select numeric variables
ORIGINAL_is.numeric <- ORIGINAL %>% 
  select_if(is.numeric)

ORIGINAL_heatmap <- round(cor(ORIGINAL_is.numeric, use = "complete.obs"), 1)
ORIGINAL_heatmap <- melt(ORIGINAL_heatmap)

#  corrplot
ggplot(ORIGINAL_heatmap, aes(x=Var1, y=Var2, fill=value)) +
  geom_tile()+
  scale_fill_gradient(low = "pink", high = "dark red")+
  labs(title="Correlation Heatmap",x="", y="")+
  geom_text(aes(x=Var1, y=Var2, label=value), color="white")+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1, size = 10), plot.title = element_text(hjust=0.5, size=10))
```




# References

[Logistic Regression in Machine Learning](https://www.javatpoint.com/logistic-regression-in-machine-learning)

[Convergence Error in Logistic Regression](https://stackoverflow.com/questions/8596160/why-am-i-getting-algorithm-did-not-converge-and-fitted-prob-numerically-0-or)

[Penalized Logistic Regression Essentials in R: Ridge, Lasso and Elastic Net](http://www.sthda.com/english/articles/36-classification-methods-essentials/149-penalized-logistic-regression-essentials-in-r-ridge-lasso-and-elastic-net/)

[Lasso Regression in R (Step-by-Step)](https://www.statology.org/lasso-regression-in-r/)

[ROC Curve](https://www.digitalocean.com/community/tutorials/plot-roc-curve-r-programming)

