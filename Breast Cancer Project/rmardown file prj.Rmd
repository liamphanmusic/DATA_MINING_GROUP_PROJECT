---
title: "Breast Cancer Project"
author: "Manuela Giansante, Liam Phan, MIcheal Bingler"
date: "2022-12-01"
output: html_document
---

```{r setup, include=FALSE, set.seed(1)}

knitr::opts_chunk$set(echo = TRUE,
                      cache = TRUE)
rm(list = ls())

```

```{r Libraries and functions, message=FALSE, warning= FALSE}

library(data.table)
library(summarytools)
library(ggplot2)
library(data.table) # fread
library(rbokeh) # visualization of data
library(summarytools) # ORIGINALSummary
library(adabag) # bagging and boosting
library(caret) # pre-processing
library(dplyr) # select, mutate_if
library(fastDummies) # dummy_cols
library(splitTools) # data partition
library(rpart) # classification tree
library(rpart.plot) # plot regression trees
library(DT) # datatable
library(corrplot) # corrplot
library(gains) # gain
library(randomForest) # randomForest
library(cluster) # hierarchical clustering
library(knitr) # kable
library(kableExtra) # kbl
library(ggplot2) # ggplot
library(MASS) # lda, qda, etc.
library(dplyr)
library(klaR) # partimat


```

```{r}

# Confusion matrix
draw_confusion_matrix <- function(cm) {
  layout(matrix(c(1,1,2)))
  par(mar=c(2,2,2,2))
  plot(c(100, 345), c(300, 450), type = "n", xlab="", ylab="", xaxt='n', yaxt='n')
  title('CONFUSION MATRIX', cex.main=2)

  # create the matrix 
  rect(150, 430, 240, 370, col='#3F97D0')
  text(195, 435, 'No', cex=1.2)
  rect(250, 430, 340, 370, col='#F7AD50')
  text(295, 435, 'Yes', cex=1.2)
  text(125, 370, 'Predicted', cex=1.3, srt=90, font=2)
  text(245, 450, 'Actual', cex=1.3, font=2)
  rect(150, 305, 240, 365, col='#F7AD50')
  rect(250, 305, 340, 365, col='#3F97D0')
  text(140, 400, 'No', cex=1.2, srt=90)
  text(140, 335, 'Yes', cex=1.2, srt=90)

  # add in the cm results 
  res <- as.numeric(cm$table)
  text(195, 400, res[1], cex=1.6, font=2, col='white')
  text(195, 335, res[2], cex=1.6, font=2, col='white')
  text(295, 400, res[3], cex=1.6, font=2, col='white')
  text(295, 335, res[4], cex=1.6, font=2, col='white')

  # add in the specifics 
  plot(c(100, 0), c(100, 0), type = "n", xlab="", ylab="", main = "DETAILS", xaxt='n', yaxt='n')
  text(5, 85, names(cm$byClass[1]), cex=1.2, font=2)
  text(5, 70, round(as.numeric(cm$byClass[1]), 3), cex=1.2)
  text(23, 85, names(cm$byClass[2]), cex=1.2, font=2)
  text(23, 70, round(as.numeric(cm$byClass[2]), 3), cex=1.2)
  text(41, 85, names(cm$byClass[5]), cex=1.2, font=2)
  text(41, 70, round(as.numeric(cm$byClass[5]), 3), cex=1.2)
  text(59, 85, names(cm$byClass[6]), cex=1.2, font=2)
  text(59, 70, round(as.numeric(cm$byClass[6]), 3), cex=1.2)
  text(77, 85, names(cm$byClass[7]), cex=1.2, font=2)
  text(77, 70, round(as.numeric(cm$byClass[7]), 3), cex=1.2)
  text(95, 85, names(cm$byClass[8]), cex=1.2, font=2)
  text(95, 70, round(as.numeric(cm$byClass[8]), 3), cex=1.2)

  # add in the accuracy information 
  text(30, 35, names(cm$overall[1]), cex=1.5, font=2)
  text(30, 20, round(as.numeric(cm$overall[1]), 3), cex=1.4)
  text(70, 35, names(cm$overall[2]), cex=1.5, font=2)
  text(70, 20, round(as.numeric(cm$overall[2]), 3), cex=1.4)
}  

```

# Data Analysis

## Structure of Data

```{r Data reading}

ORIGINAL <- fread("data/Breast_Cancer/breast-cancer.csv")

# printing sumamry
print(dfSummary(ORIGINAL, valid.col = FALSE, graph.magnif = 0.75, plain.ascii = FALSE, html = TRUE, style ='grid', silent = TRUE), max.tbl.height = 300, width = 80, method = "render")

```

## Missing values

## Distribution of Data

```{r boxplots}

norm.value <- preProcess(ORIGINAL, method = c("center", "scale"))
ORIGINAL.boxplot <- predict(norm.value, ORIGINAL)
ORIGINAL.boxplot <-  melt(select(ORIGINAL.boxplot, -c(id)))

library(ggplot2)
ggplot(ORIGINAL.boxplot, aes(x = diagnosis, y = value)) +
  facet_wrap(~variable) + 
  stat_boxplot(geom ='errorbar') + 
  geom_boxplot()

```

## Correlation

```{r corrplot}

# select numeric variables
ORIGINAL_is.numeric <- ORIGINAL %>% 
  select_if(is.numeric)

ORIGINAL_heatmap <- round(cor(ORIGINAL_is.numeric, use = "complete.obs"), 1)
ORIGINAL_heatmap <- melt(ORIGINAL_heatmap)

#  corrplot
ggplot(ORIGINAL_heatmap, aes(x=Var1, y=Var2, fill=value)) +
  geom_tile()+
  scale_fill_gradient(low = "pink", high = "dark red")+
  labs(title="Correlation Heatmap",x="", y="")+
  geom_text(aes(x=Var1, y=Var2, label=value), color="white")+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1), plot.title = element_text(hjust=0.5))

```

# Data Preparation

## General 

## Transformation

```{r Transform Outcome Variable to Binary Factor Variable}

# Transform Character Format to Binary Numerical Values on Outcome Variable "Diagnosis"

ORIGINAL[diagnosis == "M", c("diagnosis")] <- 1  # 1 for Malign Outcome
ORIGINAL[diagnosis == "B", c("diagnosis")] <- 0  # 0 for Benign Outcome 

ORIGINAL$diagnosis <- as.factor(ORIGINAL$diagnosis) # To Factor Variable 

```


## Standardization

ORIGINAL
ORIGINAL center scale
ORIGINAL range

## Partitioning 

```{r Partitioning the Dataset}

splitting <- sample(1:3,size=nrow(ORIGINAL),replace=TRUE,prob=c(0.5,0.3,0.2))
Training <- ORIGINAL[splitting==1,]
Validation <- ORIGINAL[splitting==2,]
Test <- ORIGINAL[splitting==3,]

# Checking if proportions are right
Prop_Training <- (nrow(Training)/nrow(ORIGINAL))*100
Prop_Validation <- (nrow(Validation)/nrow(ORIGINAL))*100
Prop_Test <- (nrow(Test)/nrow(ORIGINAL))*100

# Print Proportion
paste("The Proportions are:", round(Prop_Training,2),"% In Training,",round(Prop_Validation,2),"% In Validation, and ",round(Prop_Test,2),"% In Test")

```

TRAINING 50, val 30, TEST 20

# Supervised Learning

## Logistic Regression

Assumptions for Logistic Regression: 

  1. The dependent variable must be categorical in nature.
  2. The independent variable should not have multi-collinearity.
  
Type of Logistic Regression: 

  1. **Binomial** (we will use this type since we only have binary outcomes, malign or benign)
  2. Multinational 
  3. Ordinal 

### Fit the Logistic Regression Model 

```{r Fit Logistic Regression}

Training_Logistic <- Training
Validation_Logistic <- Validation

Training_Logistic <- Training_Logistic[,-c("id")]
Validation_Logistic <- Validation_Logistic[,-c("id")]

#fit logistic regression model
Logistic_Model_1 <- glm(Training_Logistic$diagnosis ~ ., family="binomial", data = Training_Logistic)

#disable scientific notation for model summary
options(scipen=999)

#view model summary
summary(Logistic_Model_1)



```



### big

### stepwise front / back / both / exhaustive search 

## Classification Tree

## K-Nearest Neighbor

```{r data prep knn}

ORIGINAL.KNN <- dplyr::select(ORIGINAL, -c("id"))
ORIGINAL.KNN$diagnosis <- factor(ifelse(ORIGINAL$diagnosis == "M", 1, 0))

# partition
inds <- partition(ORIGINAL.KNN$diagnosis, p = c(train = 0.5, valid = 0.3, test = 0.2))
ORIGINAL.KNN.train <- ORIGINAL.KNN[inds$train]
ORIGINAL.KNN.valid <- ORIGINAL.KNN[inds$valid]
ORIGINAL.KNN.test <- ORIGINAL.KNN[inds$test]

# standardize
norm.value <- preProcess(ORIGINAL.KNN.train, method = c("center", "scale"))
ORIGINAL.KNN.train <- predict(norm.value, ORIGINAL.KNN.train)
ORIGINAL.KNN.valid <- predict(norm.value, ORIGINAL.KNN.valid)
ORIGINAL.KNN.test <- predict(norm.value, ORIGINAL.KNN.test)

```

best k 

```{r knn best k}
accuracy.df <- data.frame(k = seq(1, 30, 1), accuracy = rep(0, 30))

# iterating over different k's
for(i in 1:30){
  # nearest neighbor
  KNN1 <- knn3(y = ORIGINAL.KNN.train$diagnosis, x = dplyr::select(ORIGINAL.KNN.train, -c(diagnosis)), k = i)

  # predictions response 
  KNN1.pred.valid.resp <- predict(KNN1, dplyr::select(ORIGINAL.KNN.valid, -c(diagnosis)), type = "class")
  
  # predictions prob 
  KNN1.pred.valid.prob <- predict(KNN1, dplyr::select(ORIGINAL.KNN.valid, -c(diagnosis)), type = "prob")[,2]
  
  # Confusionmatrix
  accuracy.df[i, 2] <- confusionMatrix(KNN1.pred.valid.resp, ORIGINAL.KNN.valid$diagnosis, positive = "1")$overall[1]
}

# table in markdown
datatable(accuracy.df)

# plot the k's 
ggplot(accuracy.df) +
 aes(x = k, y = accuracy) +
 geom_line(size = 0.7, colour = "#112646") +
 labs(x = "Number of k nearest neighbours", 
 y = "Accuracy", title = "Accuracy regarding k") +
 theme_minimal()
```

calculate best knn

```{r best knn}
# nearest neighbor
KNN1 <- knn3(y = ORIGINAL.KNN.train$diagnosis, x = dplyr::select(ORIGINAL.KNN.train, -c(diagnosis)), k = 3)

# predictions response 
KNN1.pred.valid.resp <- predict(KNN1, dplyr::select(ORIGINAL.KNN.valid, -c(diagnosis)), type = "class")

# predictions prob 
KNN1.pred.valid.prob <- predict(KNN1, dplyr::select(ORIGINAL.KNN.valid, -c(diagnosis)), type = "prob")[,2]

# Confusionmatrix
KNN1.conf.mat <- confusionMatrix(KNN1.pred.valid.resp, ORIGINAL.KNN.valid$diagnosis, positive = "1")
draw_confusion_matrix(KNN1.conf.mat)
```

## Neural Networks

## Discriminant Analysis

First I just run a discriminant analysis.

First data prep before model -> center / scale, 

```{r DA data prep}

ORIGINAL.DA <- dplyr::select(ORIGINAL, -c("id"))
ORIGINAL.DA$diagnosis <- factor(ifelse(ORIGINAL$diagnosis == "M", 1, 0))

# partition
inds <- partition(ORIGINAL.DA$diagnosis, p = c(train = 0.5, valid = 0.3, test = 0.2))
ORIGINAL.DA.train <- ORIGINAL.DA[inds$train]
ORIGINAL.DA.valid <- ORIGINAL.DA[inds$valid]
ORIGINAL.DA.test <- ORIGINAL.DA[inds$test]

# standardize
norm.value <- preProcess(ORIGINAL.DA.train, method = c("center", "scale"))
ORIGINAL.DA.train <- predict(norm.value, ORIGINAL.DA.train)
ORIGINAL.DA.valid <- predict(norm.value, ORIGINAL.DA.valid)
ORIGINAL.DA.test <- predict(norm.value, ORIGINAL.DA.test)

```

try with lda

```{r first DA}

# Fit the model
DA1 <- lda(diagnosis~., data = ORIGINAL.DA.train)

# Make predictions
predictions <- predict(DA1, ORIGINAL.DA.valid)

# predictions prob
DA1.pred.valid.prob <- predictions$posterior[,2]

# predictions response 
DA1.pred.valid.resp <- factor(predictions$class)

# confusion matrix
DA1.conf.mat <- confusionMatrix(DA1.pred.valid.resp, ORIGINAL.DA.valid$diagnosis, positive = "1")
draw_confusion_matrix(DA1.conf.mat)

# Lift chart
actual <- as.numeric(as.character(ORIGINAL.DA.valid$diagnosis))
prob <- as.numeric(DA1.pred.valid.prob)
DA1.gain <- gains(actual, prob)
barplot(DA1.gain$mean.resp / mean(actual), names.arg = DA1.gain$depth, xlab = "Percentile",
ylab = "Mean Response", main = "Decile-wise lift chart")

# Evaluating LDA
# doesn't work
# klaR::partimat(diagnosis~., data = ORIGINAL.DA.train, method="lda", plot.matrix = FALSE)

```

If bad -> Correlation in data -> qda continuing with it. If not continue with lda

```{r qda}

# Fit the model
DA2 <- qda(diagnosis~., data = ORIGINAL.DA.train)

# Make predictions
predictions <- predict(DA2, ORIGINAL.DA.valid)

# predictions prob
DA2.pred.valid.prob <- predictions$posterior[,2]

# predictions response 
DA2.pred.valid.resp <- factor(predictions$class)

# confusion matrix
DA2.conf.mat <- confusionMatrix(DA2.pred.valid.resp, ORIGINAL.DA.valid$diagnosis, positive = "1")
draw_confusion_matrix(DA2.conf.mat)

# Lift chart
actual <- as.numeric(as.character(ORIGINAL.DA.valid$diagnosis))
prob <- as.numeric(DA2.pred.valid.prob)
DA2.gain <- gains(actual, prob)
barplot(DA2.gain$mean.resp / mean(actual), names.arg = DA2.gain$depth, xlab = "Percentile",
ylab = "Mean Response", main = "Decile-wise lift chart")

```

Doesn't get better... lets continue with lda

We see different proportions -> need to refit model with cost to proportion

```{r lda different proportions}

# Fit the model
DA3 <- lda(diagnosis~., data = ORIGINAL.DA.train)

# Make predictions
predictions <- predict(DA2, ORIGINAL.DA.valid)

# predictions prob
DA2.pred.valid.prob <- predictions$posterior[,2]

# predictions response 
DA2.pred.valid.resp <- factor(predictions$class)

# confusion matrix
DA2.conf.mat <- confusionMatrix(DA2.pred.valid.resp, ORIGINAL.DA.valid$diagnosis, positive = "1")
draw_confusion_matrix(DA2.conf.mat)

# Lift chart
actual <- as.numeric(as.character(ORIGINAL.DA.valid$diagnosis))
prob <- as.numeric(DA2.pred.valid.prob)
DA2.gain <- gains(actual, prob)
barplot(DA2.gain$mean.resp / mean(actual), names.arg = DA2.gain$depth, xlab = "Percentile",
ylab = "Mean Response", main = "Decile-wise lift chart")

```

we know missclassification bad -> cost for misclassification model

3rd model

We see that outliers make it bad -> remove some rows

4th model

Some predictors unimportant -> remove some

5th model

```{r data prep for DA}

# remove outliers
# use a different filter than boxplot?
# https://statsandr.com/blog/outliers-detection-in-r/

ORIGINAL.DA <- select(ORIGINAL.DA, -c(id))



# normality -> log transform

# correlation between groups

```


```{r base model}

```

## Ensemble Methods

### bagging

### boosting

### rf

### majority vote -> combined

### average of models -> combined

## Comparision of Models

# Unsupervised Learning

## Cluster Analysis

# Conclusion

```{r Data reading}

ORIGINAL <- fread("data/Breast_Cancer/breast-cancer.csv")

# printing sumamry
print(ORIGINALSummary(ORIGINAL, valid.col = FALSE, graph.magnif = 0.75, plain.ascii = FALSE, html = TRUE, style ='grid', silent = TRUE), max.tbl.height = 300, width = 80, method = "render")

```

```{r corrplot, fig.width=19,fig.height=19}
# select numeric variables
ORIGINAL_is.numeric <- ORIGINAL %>% 
  select_if(is.numeric)

ORIGINAL_heatmap <- round(cor(ORIGINAL_is.numeric, use = "complete.obs"), 1)
ORIGINAL_heatmap <- melt(ORIGINAL_heatmap)

#  corrplot
ggplot(ORIGINAL_heatmap, aes(x=Var1, y=Var2, fill=value)) +
  geom_tile()+
  scale_fill_gradient(low = "pink", high = "dark red")+
  labs(title="Correlation Heatmap",x="", y="")+
  geom_text(aes(x=Var1, y=Var2, label=value), color="white")+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1, size = 10), plot.title = element_text(hjust=0.5, size=10))
```




# References

[Logistic Regression in Machine Learning](https://www.javatpoint.com/logistic-regression-in-machine-learning)

